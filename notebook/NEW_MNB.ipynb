{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_indonesia = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(object):\n",
    "    def __init__(self):\n",
    "        self.factory = StemmerFactory()\n",
    "        self.stemmer = self.factory.create_stemmer()\n",
    "        self.kamus = self.__get_dictionary()\n",
    "\n",
    "    def __get_dictionary(self):\n",
    "        df = pd.read_csv('normalisasi.csv', sep=';')\n",
    "        dictlist = []\n",
    "        for row in df.values:\n",
    "            dictlist.append([row[0], row[1]])\n",
    "        return dictlist\n",
    "\n",
    "    def __remove_pattern(self, tweet: str, pattern):\n",
    "        r = re.findall(pattern, tweet)\n",
    "        for i in r:\n",
    "            tweet = re.sub(i, '', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def __remove_symbol(self, tweet: str):\n",
    "        tweet = self.__remove_url(tweet)\n",
    "        # get only alfabet\n",
    "        pattern = re.compile(r'\\b[^\\d\\W]+\\b')\n",
    "        newwords = []\n",
    "        for word in pattern.findall(tweet):\n",
    "            # case folding\n",
    "\n",
    "            word = word.lower()\n",
    "            for row in self.kamus:\n",
    "                key = row[0]\n",
    "                value = row[1]\n",
    "                if word == key:\n",
    "                    word = value\n",
    "                    break\n",
    "\n",
    "            word = word.replace(\"xyz\", \"\")\n",
    "            newwords.append(word)\n",
    "        return \" \".join(newwords)\n",
    "\n",
    "    def __remove_url(self, text):\n",
    "        # Remove additional white spaces\n",
    "        text = re.sub('[\\s]+', ' ', text)\n",
    "        text = re.sub('[\\n]+', ' ', text)\n",
    "        # remove all url\n",
    "        text = re.sub(r\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", text)\n",
    "        # remove email\n",
    "        text = re.sub(r\"[\\w]+@[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # remove text twit\n",
    "        text = re.sub(r'((pic\\.[^\\s]+)|(twitter))', '', text)\n",
    "        # remove mentions, hashtag and web\n",
    "        text = re.sub(r\"(?:\\@|#|http?\\://)\\S+\", \"\", text)\n",
    "        # remove url\n",
    "        text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r'((https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r\"(pic[^\\s]+)|[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # replace non ascii\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __remove_emojis(self, data):\n",
    "        emoj = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                          u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                          u\"\\U00002702-\\U000027B0\"  \n",
    "                          u\"\\U000024C2-\\U0001F251\"\n",
    "                          u\"\\U0001f926-\\U0001f937\"\n",
    "                          u\"\\U00010000-\\U0010ffff\"\n",
    "                          u\"\\u2640-\\u2642\"\n",
    "                          u\"\\u2600-\\u2B55\"\n",
    "                          u\"\\u200d\"\n",
    "                          u\"\\u23cf\"\n",
    "                          u\"\\u23e9\"\n",
    "                          u\"\\u231a\"\n",
    "                          u\"\\ufe0f\"  # dingbats\n",
    "                          u\"\\u3030\"\n",
    "                          \"]+\", re.UNICODE)\n",
    "        return re.sub(emoj, '', data)\n",
    "    \n",
    "    def __concate_duplicate(self, tweet):\n",
    "        term = \"a\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", tweet)\n",
    "        term = \"i\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"u\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"e\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"o\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        term = \"c\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"k\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"w\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"h\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        return rep\n",
    "\n",
    "    def __clean_tweets(self, tweet: str) -> str:\n",
    "        # tokenize tweets\n",
    "        \n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        \n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        \n",
    "        tweets_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in stopwords_indonesia and  # remove stopwords\n",
    "                    word not in string.punctuation):  # remove punctuation\n",
    "                tweets_clean.append(word)\n",
    "\n",
    "        stem_word = self.stemmer.stem(\" \".join(tweets_clean))  # stemming word\n",
    "        return stem_word\n",
    "\n",
    "    def from_csv(self, file_name):\n",
    "        raw_data = pd.read_csv(file_name)\n",
    "#         df = pd.DataFrame(raw_data[['user_account', 'tweet', 'label']])\n",
    "        df = pd.DataFrame(raw_data[['Handle', 'Text']])\n",
    "\n",
    "        df['remove_user'] = np.vectorize(self.__remove_pattern)(df['Text'], \"(@\\\\w*)\")\n",
    "        df['remove_symbol'] = df[\"remove_user\"].apply(lambda x: np.vectorize(self.__remove_pattern)(x, \"(#\\\\w*)\"))\n",
    "        df['remove_duplicate_char'] = df['remove_symbol'].apply(self.__concate_duplicate)\n",
    "        df['remove_emojis'] = df['remove_duplicate_char'].apply(lambda x: self.__remove_emojis(self.__remove_symbol(x)))\n",
    "        \n",
    "        df.drop_duplicates(subset=\"remove_emojis\", keep='first', inplace=True)\n",
    "        \n",
    "        df['tweet_clean'] = df['remove_emojis'].apply(lambda x: self.__clean_tweets(x))\n",
    "        df = df.dropna(subset=[\"tweet_clean\"])\n",
    "        for i, row in df.iterrows():\n",
    "            if row['tweet_clean'] == \"\":\n",
    "                df = df.drop(i)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = 'sentimen_dataset.csv'\n",
    "preprocessing = Preprocessing()\n",
    "dataset = preprocessing.from_csv(raw_data)\n",
    "# dataset.to_csv('new_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['ruu'] = dataset['tweet_clean'].str.contains('rancang undang undang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in dataset.iterrows():\n",
    "#             if row['ruu'] == False:\n",
    "#                 dataset = dataset.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.drop_duplicates(subset=\"tweet_clean\", keep='first', inplace=True)\n",
    "# dataset.to_csv('news_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    analisis = TextBlob(text)\n",
    "    an = analisis.translate(from_lang='id',to='en')\n",
    "    if an.sentiment.polarity >= 0.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator, LANGUAGES\n",
    "\n",
    "def translate(text):\n",
    "    translator = Translator(service_urls=['translate.google.com'])\n",
    "    lang_en = translator.translate(text)\n",
    "    return LANGUAGES[lang_en.dest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-488323071d1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bhs_inggris'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-488323071d1f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(tweet)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bhs_inggris'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-61-284eef15db4c>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtranslator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice_urls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'translate.google.com'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mlang_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mLANGUAGES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlang_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0morigin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;31m# this code will be updated when the format is changed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36m_translate\u001b[1;34m(self, text, dest, src, override)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[0;32m     80\u001b[0m                                     token=token, override=override)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         \u001b[0mtk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\googletrans\\gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'var '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unicode-escape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "dataset['bhs_inggris'] = dataset['tweet_clean'].apply(lambda tweet: translate(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('new_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.drop(['remove_user', 'remove_symbol', 'remove_duplicate_char', 'remove_emojis', 'tweet_clean', 'ruu'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_csv('sentimen_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfFeature(object):\n",
    " \n",
    "    def __init__(self):\n",
    "        self.tf_dict = {}\n",
    "        self.idf_dict = {}\n",
    "        \n",
    "\n",
    "    def __tokenize(self, tweet):\n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        return tokenizer.tokenize(tweet)\n",
    "\n",
    "    def __calc_TF_Dict(self, document):\n",
    "        TF_dict = {}\n",
    "        for term in document:\n",
    "            if term in TF_dict:\n",
    "                TF_dict[term] += 1\n",
    "            else:\n",
    "                TF_dict[term] = 1\n",
    "        return TF_dict\n",
    "\n",
    "    def __calc_count_Dict(self, tfDict):\n",
    "        count_DF = {}\n",
    "        for document in tfDict:\n",
    "            for term in document:\n",
    "                if term in count_DF:\n",
    "                    count_DF[term] += 1\n",
    "                else:\n",
    "                    count_DF[term] = 1\n",
    "        return count_DF\n",
    "\n",
    "    def __calc_IDF_Dict(self, __n_document, __DF):\n",
    "        IDF_Dict = {}\n",
    "        for term in __DF:\n",
    "            IDF_Dict[term] = np.log(__n_document / __DF[term])\n",
    "        return IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF(self, TF):\n",
    "        TF_IDF_Dict = {}\n",
    "        for key in TF:\n",
    "            TF_IDF_Dict[key] = self.tf_dict[key] * self.idf_dict[key]\n",
    "        return TF_IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF_Vec(self, __TF_IDF_Dict):\n",
    "        wordDict = sorted(self.tf_dict.keys())\n",
    "        TF_IDF_vector = [0.0] * len(wordDict)\n",
    "\n",
    "        for i, term in enumerate(wordDict):\n",
    "            if term in __TF_IDF_Dict:\n",
    "                TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "        return TF_IDF_vector\n",
    "\n",
    "    def set_tf_idf_dict(self, data):\n",
    "        data['tweet_token'] = data['tweet_clean'].apply(self.__tokenize)\n",
    "        data[\"tf_dict\"] = data['tweet_token'].apply(self.__calc_TF_Dict)\n",
    "        self.tf_dict = self.__calc_count_Dict(data[\"tf_dict\"])\n",
    "        self.idf_dict = self.__calc_IDF_Dict(len(data),  self.tf_dict)\n",
    "\n",
    "    def calc_tf_idf(self, data):\n",
    "        data_token = data.apply(self.__tokenize)\n",
    "        data_tf_dict = data_token.apply(self.__calc_TF_Dict)\n",
    "        data_tfidf_dict = data_tf_dict.apply(self.__calc_TF_IDF)\n",
    "        tfidf_vector = [self.__calc_TF_IDF_Vec(row) for row in data_tfidf_dict]\n",
    "        return tfidf_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = TfidfFeature()\n",
    "feature.set_tf_idf_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['tweet_clean'], dataset['sentiment'], test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train = feature.calc_tf_idf(x_train)\n",
    "ft_test = feature.calc_tf_idf(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _predict(self, x_test):\n",
    "        # Calculate posterior for each class\n",
    "        posteriors = []\n",
    "        for idx, _ in enumerate(self._classes):\n",
    "            prior_c = np.log(self._priors[idx])\n",
    "            conditionals_c = self._calc_conditionals(\n",
    "                self._conditionals[idx, :], x_test)\n",
    "            posteriors_c = np.sum(conditionals_c) + prior_c\n",
    "            posteriors.append(posteriors_c)\n",
    "\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _calc_conditionals(self, cls_cond, x_test):\n",
    "        return np.log(cls_cond) * x_test\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        m, n = X_train.shape\n",
    "        self._classes = np.unique(y_train)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # init: Prior & Conditional\n",
    "        self._priors = np.zeros(n_classes)\n",
    "        self._conditionals = np.zeros((n_classes, n))\n",
    "\n",
    "        # Get Prior and Conditional\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_train_c = X_train[c == y_train]\n",
    "            self._priors[idx] = X_train_c.shape[0] / m\n",
    "            self._conditionals[idx, :] = ((X_train_c.sum(axis=0)) + self.alpha) / (np.sum(X_train_c.sum(axis=0) + self.alpha))\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return [self._predict(x_test) for x_test in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(ft_train, y_train)\n",
    "predict = nb.predict(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confus = confusion_matrix(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confus.ravel()\n",
    "accuracy = ((tp + tn)/(tp + tn + fp + fn))*100\n",
    "precision = (tp / (tp + fp))*100\n",
    "recall = (tp / (tp + fn))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 90.1048492791612\n",
      "precision = 3.90625\n",
      "recall = 15.151515151515152\n"
     ]
    }
   ],
   "source": [
    "print('accuracy =', accuracy)\n",
    "print('precision =', precision)\n",
    "print('recall =', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
