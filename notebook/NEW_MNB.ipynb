{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_indonesia = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(object):\n",
    "    def __init__(self):\n",
    "        self.factory = StemmerFactory()\n",
    "        self.stemmer = self.factory.create_stemmer()\n",
    "        self.kamus = self.__get_dictionary()\n",
    "\n",
    "    def __get_dictionary(self):\n",
    "        df = pd.read_csv('normalisasi.csv', sep=';')\n",
    "        dictlist = []\n",
    "        for row in df.values:\n",
    "            dictlist.append([row[0], row[1]])\n",
    "        return dictlist\n",
    "\n",
    "    def __remove_pattern(self, tweet: str, pattern):\n",
    "        r = re.findall(pattern, tweet)\n",
    "        for i in r:\n",
    "            tweet = re.sub(i, '', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def __remove_symbol(self, tweet: str):\n",
    "        tweet = self.__remove_url(tweet)\n",
    "        # get only alfabet\n",
    "        pattern = re.compile(r'\\b[^\\d\\W]+\\b')\n",
    "        newwords = []\n",
    "        for word in pattern.findall(tweet):\n",
    "            # case folding\n",
    "\n",
    "            word = word.lower()\n",
    "            for row in self.kamus:\n",
    "                key = row[0]\n",
    "                value = row[1]\n",
    "                if word == key:\n",
    "                    word = value\n",
    "                    break\n",
    "\n",
    "            word = word.replace(\"xyz\", \"\")\n",
    "            newwords.append(word)\n",
    "        return \" \".join(newwords)\n",
    "\n",
    "    def __remove_url(self, text):\n",
    "        # Remove additional white spaces\n",
    "        text = re.sub('[\\s]+', ' ', text)\n",
    "        text = re.sub('[\\n]+', ' ', text)\n",
    "        # remove all url\n",
    "        text = re.sub(r\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", text)\n",
    "        # remove email\n",
    "        text = re.sub(r\"[\\w]+@[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # remove text twit\n",
    "        text = re.sub(r'((pic\\.[^\\s]+)|(twitter))', '', text)\n",
    "        # remove mentions, hashtag and web\n",
    "        text = re.sub(r\"(?:\\@|#|http?\\://)\\S+\", \"\", text)\n",
    "        # remove url\n",
    "        text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r'((https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r\"(pic[^\\s]+)|[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # replace non ascii\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __remove_emojis(self, data):\n",
    "        emoj = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                          u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                          u\"\\U00002702-\\U000027B0\"  \n",
    "                          u\"\\U000024C2-\\U0001F251\"\n",
    "                          u\"\\U0001f926-\\U0001f937\"\n",
    "                          u\"\\U00010000-\\U0010ffff\"\n",
    "                          u\"\\u2640-\\u2642\"\n",
    "                          u\"\\u2600-\\u2B55\"\n",
    "                          u\"\\u200d\"\n",
    "                          u\"\\u23cf\"\n",
    "                          u\"\\u23e9\"\n",
    "                          u\"\\u231a\"\n",
    "                          u\"\\ufe0f\"  # dingbats\n",
    "                          u\"\\u3030\"\n",
    "                          \"]+\", re.UNICODE)\n",
    "        return re.sub(emoj, '', data)\n",
    "    \n",
    "    def __concate_duplicate(self, tweet):\n",
    "        term = \"a\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", tweet)\n",
    "        term = \"i\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"u\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"e\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"o\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        term = \"c\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"k\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"w\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"h\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        return rep\n",
    "\n",
    "    def __clean_tweets(self, tweet: str) -> str:\n",
    "        # tokenize tweets\n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "        tweets_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in stopwords_indonesia and  # remove stopwords\n",
    "                    word not in string.punctuation):  # remove punctuation\n",
    "                tweets_clean.append(word)\n",
    "\n",
    "        stem_word = self.stemmer.stem(\" \".join(tweets_clean))  # stemming word\n",
    "        return stem_word\n",
    "\n",
    "    def from_csv(self, file_name):\n",
    "        raw_data = pd.read_csv(file_name)\n",
    "        df = pd.DataFrame(raw_data[['user_account', 'tweet', 'label']])\n",
    "\n",
    "        df['remove_user'] = np.vectorize(self.__remove_pattern)(df['tweet'], \"(@\\\\w*)\")\n",
    "        df['remove_symbol'] = df[\"remove_user\"].apply(lambda x: np.vectorize(self.__remove_pattern)(x, \"(#\\\\w*)\"))\n",
    "        df['remove_duplicate_char'] = df['remove_symbol'].apply(self.__concate_duplicate)\n",
    "        df['remove_emojis'] = df['remove_duplicate_char'].apply(lambda x: self.__remove_emojis(self.__remove_symbol(x)))\n",
    "        \n",
    "        df.drop_duplicates(subset=\"remove_emojis\", keep='first', inplace=True)\n",
    "        \n",
    "        df['tweet_clean'] = df['remove_emojis'].apply(lambda x: self.__clean_tweets(x))\n",
    "        df = df.dropna(subset=[\"label\", \"tweet_clean\"])\n",
    "        for i, row in df.iterrows():\n",
    "            if row['tweet_clean'] == \"\":\n",
    "                df = df.drop(i)\n",
    "        df.to_csv('df.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = 'dataset.min.csv'\n",
    "preprocessing = Preprocessing()\n",
    "dataset = preprocessing.from_csv(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ketua dewan wakil rakyat draf rancang undang u...\n",
       "1    jurus kilat ala sistem kebut skripsi layak mah...\n",
       "2    jatuh timpa tangga bebas covid panggang api pa...\n",
       "3    tokoh ahmad yani aku angkut polisi ahmad yani ...\n",
       "Name: tweet_clean, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['tweet_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfFeature(object):\n",
    " \n",
    "    def __init__(self):\n",
    "        self.tf_dict = {}\n",
    "        self.idf_dict = {}\n",
    "        \n",
    "\n",
    "    def __tokenize(self, tweet):\n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        return tokenizer.tokenize(tweet)\n",
    "\n",
    "    def __calc_TF_Dict(self, document):\n",
    "        TF_dict = {}\n",
    "        for term in document:\n",
    "            if term in TF_dict:\n",
    "                TF_dict[term] += 1\n",
    "            else:\n",
    "                TF_dict[term] = 1\n",
    "        return TF_dict\n",
    "\n",
    "    def __calc_count_Dict(self, tfDict):\n",
    "        count_DF = {}\n",
    "        for document in tfDict:\n",
    "            for term in document:\n",
    "                if term in count_DF:\n",
    "                    count_DF[term] += 1\n",
    "                else:\n",
    "                    count_DF[term] = 1\n",
    "        return count_DF\n",
    "\n",
    "    def __calc_IDF_Dict(self, __n_document, __DF):\n",
    "        IDF_Dict = {}\n",
    "        for term in __DF:\n",
    "            IDF_Dict[term] = np.log(__n_document / __DF[term])\n",
    "        return IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF(self, TF):\n",
    "        TF_IDF_Dict = {}\n",
    "        for key in TF:\n",
    "            TF_IDF_Dict[key] = self.tf_dict[key] * self.idf_dict[key]\n",
    "        return TF_IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF_Vec(self, __TF_IDF_Dict):\n",
    "        wordDict = sorted(self.tf_dict.keys())\n",
    "        TF_IDF_vector = [0.0] * len(wordDict)\n",
    "\n",
    "        for i, term in enumerate(wordDict):\n",
    "            if term in __TF_IDF_Dict:\n",
    "                TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "        return TF_IDF_vector\n",
    "\n",
    "    def set_tf_idf_dict(self, data):\n",
    "        data['tweet_token'] = data['tweet_clean'].apply(self.__tokenize)\n",
    "        data[\"tf_dict\"] = data['tweet_token'].apply(self.__calc_TF_Dict)\n",
    "        self.tf_dict = self.__calc_count_Dict(data[\"tf_dict\"])\n",
    "        self.idf_dict = self.__calc_IDF_Dict(len(data),  self.tf_dict)\n",
    "\n",
    "    def calc_tf_idf(self, data):\n",
    "        data_token = data.apply(self.__tokenize)\n",
    "        data_tf_dict = data_token.apply(self.__calc_TF_Dict)\n",
    "        data_tfidf_dict = data_tf_dict.apply(self.__calc_TF_IDF)\n",
    "        tfidf_vector = [self.__calc_TF_IDF_Vec(row) for row in data_tfidf_dict]\n",
    "        return tfidf_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = TfidfFeature()\n",
    "feature.set_tf_idf_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['tweet_clean'], dataset['label'], test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train = feature.calc_tf_idf(x_train)\n",
    "ft_test = feature.calc_tf_idf(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.8630462173553426,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8630462173553426,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.8630462173553426,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8630462173553426,\n",
       "  1.3862943611198906,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8630462173553426,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.8630462173553426,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8630462173553426,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.8630462173553426,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.8630462173553426,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8630462173553426,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8630462173553426,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.3862943611198906,\n",
       "  1.3862943611198906,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.8630462173553426,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _predict(self, x_test):\n",
    "        # Calculate posterior for each class\n",
    "        posteriors = []\n",
    "        for idx, _ in enumerate(self._classes):\n",
    "            prior_c = np.log(self._priors[idx])\n",
    "            conditionals_c = self._calc_likelihood(\n",
    "                self._conditionals[idx, :], x_test)\n",
    "            posteriors_c = np.sum(conditionals_c) + prior_c\n",
    "            posteriors.append(posteriors_c)\n",
    "            print('x_test = ', np.log(self._conditionals[idx, :]) * x_test)\n",
    "\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _calc_likelihood(self, cls_likeli, x_test):\n",
    "        return np.log(cls_likeli) * x_test\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        print('X_train = ', X_train)\n",
    "        m, n = X_train.shape\n",
    "        self._classes = np.unique(y_train)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # init: Prior & Conditional\n",
    "        self._priors = np.zeros(n_classes)\n",
    "        self._conditionals = np.zeros((n_classes, n))\n",
    "\n",
    "        # Get Prior and Conditional\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_train_c = X_train[c == y_train]\n",
    "            print('X_train_c = ', X_train_c)\n",
    "            print('X_train_c.shape[0]', X_train_c.shape[0])\n",
    "            self._priors[idx] = X_train_c.shape[0] / m\n",
    "            print('self._priors[idx] = ', self._priors[idx])\n",
    "            self._conditionals[idx, :] = ((X_train_c.sum(axis=0)) + self.alpha) / (np.sum(X_train_c.sum(axis=0) + self.alpha))\n",
    "            print('self._conditionals = ', self._conditionals[idx, :])\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return [self._predict(x_test) for x_test in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train =  [[1.38629436 0.         0.         0.         0.         0.\n",
      "  0.         1.38629436 0.         1.38629436 1.38629436 0.\n",
      "  0.86304622 0.         0.         1.38629436 1.38629436 1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.86304622\n",
      "  1.38629436 0.         1.38629436 0.         0.         0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 0.\n",
      "  0.         1.38629436 1.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         1.38629436 0.\n",
      "  0.         1.38629436 1.38629436 0.86304622 1.38629436 0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.86304622 1.38629436 0.        ]\n",
      " [0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.86304622 0.         0.         0.         0.         0.\n",
      "  0.         1.38629436 0.         1.38629436 0.         0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 0.86304622\n",
      "  0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         1.38629436 1.38629436 1.38629436 0.         0.\n",
      "  0.         0.         0.         0.         1.38629436 1.38629436\n",
      "  0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         0.         0.         0.86304622 0.         0.\n",
      "  1.38629436 1.38629436 1.38629436 1.38629436 0.         0.\n",
      "  1.38629436 1.38629436 1.38629436 0.         0.         0.\n",
      "  1.38629436 0.86304622 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.38629436 0.         1.38629436 0.         0.         1.38629436\n",
      "  0.86304622 1.38629436 0.         0.         0.         0.\n",
      "  0.         0.         1.38629436 0.         0.         1.38629436\n",
      "  0.         0.         1.38629436 0.         0.         0.86304622\n",
      "  1.38629436 1.38629436 0.         0.         1.38629436 0.\n",
      "  0.         0.         0.         0.         0.         1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.38629436 1.38629436 0.         0.         1.38629436\n",
      "  0.         0.         0.         0.86304622 0.         1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.38629436 1.38629436 0.\n",
      "  0.         0.86304622 0.         0.        ]]\n",
      "X_train_c =  [[0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.86304622 0.         0.         0.         0.         0.\n",
      "  0.         1.38629436 0.         1.38629436 0.         0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 0.86304622\n",
      "  0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         1.38629436 1.38629436 1.38629436 0.         0.\n",
      "  0.         0.         0.         0.         1.38629436 1.38629436\n",
      "  0.         0.         0.         1.38629436 0.         0.\n",
      "  0.         0.         0.         0.86304622 0.         0.\n",
      "  1.38629436 1.38629436 1.38629436 1.38629436 0.         0.\n",
      "  1.38629436 1.38629436 1.38629436 0.         0.         0.\n",
      "  1.38629436 0.86304622 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.38629436 0.         1.38629436 0.         0.         1.38629436\n",
      "  0.86304622 1.38629436 0.         0.         0.         0.\n",
      "  0.         0.         1.38629436 0.         0.         1.38629436\n",
      "  0.         0.         1.38629436 0.         0.         0.86304622\n",
      "  1.38629436 1.38629436 0.         0.         1.38629436 0.\n",
      "  0.         0.         0.         0.         0.         1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.38629436 1.38629436 0.         0.         1.38629436\n",
      "  0.         0.         0.         0.86304622 0.         1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.38629436 1.38629436 0.\n",
      "  0.         0.86304622 0.         0.        ]]\n",
      "X_train_c.shape[0] 2\n",
      "self._priors[idx] =  0.6666666666666666\n",
      "self._conditionals =  [0.00519765 0.00519765 0.00519765 0.01960861 0.00519765 0.00519765\n",
      " 0.01960861 0.00519765 0.01960861 0.00519765 0.00519765 0.01960861\n",
      " 0.02314091 0.01960861 0.00519765 0.00519765 0.00519765 0.00519765\n",
      " 0.00519765 0.01960861 0.01960861 0.01960861 0.00519765 0.01960861\n",
      " 0.01960861 0.00519765 0.01960861 0.00519765 0.01960861 0.02314091\n",
      " 0.01960861 0.01960861 0.00519765 0.01960861 0.01960861 0.00519765\n",
      " 0.00519765 0.01960861 0.01960861 0.01960861 0.00519765 0.01960861\n",
      " 0.00519765 0.00519765 0.00519765 0.00519765 0.01960861 0.01960861\n",
      " 0.00519765 0.01960861 0.01960861 0.01960861 0.00519765 0.01960861\n",
      " 0.00519765 0.00519765 0.00519765 0.02314091 0.00519765 0.01960861\n",
      " 0.01960861 0.01960861 0.01960861 0.01960861 0.00519765 0.00519765\n",
      " 0.01960861 0.01960861 0.01960861 0.01960861 0.01960861 0.00519765\n",
      " 0.01960861 0.02314091 0.00519765 0.00519765]\n",
      "X_train_c =  [[1.38629436 0.         0.         0.         0.         0.\n",
      "  0.         1.38629436 0.         1.38629436 1.38629436 0.\n",
      "  0.86304622 0.         0.         1.38629436 1.38629436 1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.86304622\n",
      "  1.38629436 0.         1.38629436 0.         0.         0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 0.\n",
      "  0.         1.38629436 1.38629436 0.         0.         0.\n",
      "  0.         0.         0.         0.         1.38629436 0.\n",
      "  0.         1.38629436 1.38629436 0.86304622 1.38629436 0.\n",
      "  1.38629436 0.         0.         0.         1.38629436 1.38629436\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.86304622 1.38629436 0.        ]]\n",
      "X_train_c.shape[0] 1\n",
      "self._priors[idx] =  0.3333333333333333\n",
      "self._conditionals =  [0.02673154 0.00708573 0.00708573 0.00708573 0.00708573 0.00708573\n",
      " 0.00708573 0.02673154 0.00708573 0.02673154 0.02673154 0.00708573\n",
      " 0.01931635 0.00708573 0.00708573 0.02673154 0.02673154 0.02673154\n",
      " 0.00708573 0.00708573 0.00708573 0.00708573 0.00708573 0.00708573\n",
      " 0.00708573 0.00708573 0.00708573 0.00708573 0.00708573 0.01931635\n",
      " 0.02673154 0.00708573 0.02673154 0.00708573 0.00708573 0.00708573\n",
      " 0.02673154 0.00708573 0.00708573 0.00708573 0.02673154 0.00708573\n",
      " 0.00708573 0.02673154 0.02673154 0.00708573 0.00708573 0.00708573\n",
      " 0.00708573 0.00708573 0.00708573 0.00708573 0.02673154 0.00708573\n",
      " 0.00708573 0.02673154 0.02673154 0.01931635 0.02673154 0.00708573\n",
      " 0.02673154 0.00708573 0.00708573 0.00708573 0.02673154 0.02673154\n",
      " 0.00708573 0.00708573 0.00708573 0.00708573 0.00708573 0.00708573\n",
      " 0.00708573 0.01931635 0.02673154 0.00708573]\n",
      "x_test =  [-0.         -7.29128183 -7.29128183 -0.         -7.29128183 -7.29128183\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -7.29128183 -0.         -0.         -0.\n",
      " -7.29128183 -0.         -0.         -0.         -7.29128183 -0.\n",
      " -0.         -7.29128183 -0.         -7.29128183 -0.         -0.\n",
      " -0.         -0.         -7.29128183 -0.         -0.         -7.29128183\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -7.29128183 -0.         -0.         -7.29128183 -0.         -0.\n",
      " -7.29128183 -0.         -0.         -0.         -0.         -0.\n",
      " -7.29128183 -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -7.29128183\n",
      " -0.         -0.         -0.         -7.29128183]\n",
      "x_test =  [-0.         -6.86170304 -6.86170304 -0.         -6.86170304 -6.86170304\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -6.86170304 -0.         -0.         -0.\n",
      " -6.86170304 -0.         -0.         -0.         -6.86170304 -0.\n",
      " -0.         -6.86170304 -0.         -6.86170304 -0.         -0.\n",
      " -0.         -0.         -5.02103486 -0.         -0.         -6.86170304\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -6.86170304 -0.         -0.         -6.86170304 -0.         -0.\n",
      " -6.86170304 -0.         -0.         -0.         -0.         -0.\n",
      " -6.86170304 -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -6.86170304\n",
      " -0.         -0.         -0.         -6.86170304]\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(ft_train, y_train)\n",
    "predict = nb.predict(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confus = confusion_matrix(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-f6c11d950abe>:4: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  recall = (tp / (tp + fn))*100\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confus.ravel()\n",
    "accuracy = ((tp + tn)/(tp + tn + fp + fn))*100\n",
    "precision = (tp / (tp + fp))*100\n",
    "recall = (tp / (tp + fn))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.0\n",
      "precision = 0.0\n",
      "recall = nan\n"
     ]
    }
   ],
   "source": [
    "print('accuracy =', accuracy)\n",
    "print('precision =', precision)\n",
    "print('recall =', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
