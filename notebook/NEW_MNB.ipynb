{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans\n",
    "from googletrans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_indonesia = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(object):\n",
    "    def __init__(self):\n",
    "        self.factory = StemmerFactory()\n",
    "        self.stemmer = self.factory.create_stemmer()\n",
    "        self.kamus = self.__get_dictionary()\n",
    "\n",
    "    def __get_dictionary(self):\n",
    "        df = pd.read_csv('normalisasi.csv', sep=';')\n",
    "        dictlist = []\n",
    "        for row in df.values:\n",
    "            dictlist.append([row[0], row[1]])\n",
    "        return dictlist\n",
    "\n",
    "    def __remove_pattern(self, tweet: str, pattern):\n",
    "        r = re.findall(pattern, tweet)\n",
    "        for i in r:\n",
    "            tweet = re.sub(i, '', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def __remove_symbol(self, tweet: str):\n",
    "        tweet = self.__remove_url(tweet)\n",
    "        # get only alfabet\n",
    "        pattern = re.compile(r'\\b[^\\d\\W]+\\b')\n",
    "        newwords = []\n",
    "        for word in pattern.findall(tweet):\n",
    "            # case folding\n",
    "\n",
    "            word = word.lower()\n",
    "            for row in self.kamus:\n",
    "                key = row[0]\n",
    "                value = row[1]\n",
    "                if word == key:\n",
    "                    word = value\n",
    "                    break\n",
    "\n",
    "            word = word.replace(\"xyz\", \"\")\n",
    "            newwords.append(word)\n",
    "        return \" \".join(newwords)\n",
    "\n",
    "    def __remove_url(self, text):\n",
    "        # Remove additional white spaces\n",
    "        text = re.sub('[\\s]+', ' ', text)\n",
    "        text = re.sub('[\\n]+', ' ', text)\n",
    "        # remove all url\n",
    "        text = re.sub(r\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", text)\n",
    "        # remove email\n",
    "        text = re.sub(r\"[\\w]+@[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # remove text twit\n",
    "        text = re.sub(r'((pic\\.[^\\s]+)|(twitter))', '', text)\n",
    "        # remove mentions, hashtag and web\n",
    "        text = re.sub(r\"(?:\\@|#|http?\\://)\\S+\", \"\", text)\n",
    "        # remove url\n",
    "        text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r'((https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r\"(pic[^\\s]+)|[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # replace non ascii\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __remove_emojis(self, data):\n",
    "        emoj = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                          u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                          u\"\\U00002702-\\U000027B0\"  \n",
    "                          u\"\\U000024C2-\\U0001F251\"\n",
    "                          u\"\\U0001f926-\\U0001f937\"\n",
    "                          u\"\\U00010000-\\U0010ffff\"\n",
    "                          u\"\\u2640-\\u2642\"\n",
    "                          u\"\\u2600-\\u2B55\"\n",
    "                          u\"\\u200d\"\n",
    "                          u\"\\u23cf\"\n",
    "                          u\"\\u23e9\"\n",
    "                          u\"\\u231a\"\n",
    "                          u\"\\ufe0f\"  # dingbats\n",
    "                          u\"\\u3030\"\n",
    "                          \"]+\", re.UNICODE)\n",
    "        return re.sub(emoj, '', data)\n",
    "    \n",
    "    def __concate_duplicate(self, tweet):\n",
    "        term = \"a\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", tweet)\n",
    "        term = \"i\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"u\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"e\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"o\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        term = \"c\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"k\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"w\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"h\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        return rep\n",
    "\n",
    "    def __clean_tweets(self, tweet: str) -> str:\n",
    "        # tokenize tweets\n",
    "        \n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        \n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        \n",
    "        tweets_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in stopwords_indonesia and  # remove stopwords\n",
    "                    word not in string.punctuation):  # remove punctuation\n",
    "                tweets_clean.append(word)\n",
    "\n",
    "        stem_word = self.stemmer.stem(\" \".join(tweets_clean))  # stemming word\n",
    "        return stem_word\n",
    "\n",
    "    def from_csv(self, file_name):\n",
    "        raw_data = pd.read_csv(file_name)\n",
    "#         df = pd.DataFrame(raw_data[['user_account', 'tweet', 'label']])\n",
    "        df = pd.DataFrame(raw_data[['Handle', 'Text']])\n",
    "\n",
    "        df['remove_user'] = np.vectorize(self.__remove_pattern)(df['Text'], \"(@\\\\w*)\")\n",
    "        df['remove_symbol'] = df[\"remove_user\"].apply(lambda x: np.vectorize(self.__remove_pattern)(x, \"(#\\\\w*)\"))\n",
    "        df['remove_duplicate_char'] = df['remove_symbol'].apply(self.__concate_duplicate)\n",
    "        df['remove_emojis'] = df['remove_duplicate_char'].apply(lambda x: self.__remove_emojis(self.__remove_symbol(x)))\n",
    "        \n",
    "        df.drop_duplicates(subset=\"remove_emojis\", keep='first', inplace=True)\n",
    "        \n",
    "        df['tweet_clean'] = df['remove_emojis'].apply(lambda x: self.__clean_tweets(x))\n",
    "        df = df.dropna(subset=[\"tweet_clean\"])\n",
    "        for i, row in df.iterrows():\n",
    "            if row['tweet_clean'] == \"\":\n",
    "                df = df.drop(i)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('sentimen_dataset.csv')\n",
    "preprocessing = Preprocessing()\n",
    "dataset = preprocessing.from_csv(raw_data)\n",
    "# dataset.to_csv('new_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('export_dataset.csv')\n",
    "df = pd.DataFrame(raw[['user_account', 'tweet', 'clean_tweet', 'sentimen']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ruu'] = df['clean_tweet'].str.contains('rancang undang undang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "            if row['ruu'] == False:\n",
    "                df = df.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_account</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>ruu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@RahmaFathiyyah</td>\n",
       "      <td>Ketua DPR: Draft RUU Cipta Kerja Hanya Dicek A...</td>\n",
       "      <td>ketua dewan wakil rakyat draf rancang undang u...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Hendria04991579</td>\n",
       "      <td>Jurus kilat ala sistem kebut skripsi layaknya ...</td>\n",
       "      <td>jurus kilat ala sistem kebut skripsi layak mah...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@anis34216465</td>\n",
       "      <td>Sudah jatuh tertimpa tangga. Bebas Covid-19 ja...</td>\n",
       "      <td>jatuh timpa tangga bebas covid panggang api pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@EkoFirman15</td>\n",
       "      <td>Mahasiswa cerdas . Pelajaran i dulu RUU CIPTA ...</td>\n",
       "      <td>mahasiswa cerdas ajar rancang undang undang ci...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@beereciel</td>\n",
       "      <td>tapi kalau topik ruu cipta kerja,,, SEBERAPA B...</td>\n",
       "      <td>topik rancang undang undang cipta kerja mampus</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>@RosidinBrawija4</td>\n",
       "      <td>ampuun dah gmna mo baca RUU cipta kerja yg set...</td>\n",
       "      <td>ampun baca rancang undang undang cipta kerja t...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>@RizalAriyadi3</td>\n",
       "      <td>RUU Cipta Kerja Putus Rantai Mafia Birokrasi !...</td>\n",
       "      <td>rancang undang undang cipta kerja putus rantai...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>@sukmariyati</td>\n",
       "      <td>rangorang di lab pada rame bahas RUU cipta ker...</td>\n",
       "      <td>orang orang lab ramai bahas rancang undang und...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>@mamaciaaa</td>\n",
       "      <td>Jadi, naskah yg dirapihkan terkait kesalahan p...</td>\n",
       "      <td>naskah rapi kait salah tulis salah ketik tanda...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>@hariankompas</td>\n",
       "      <td>Satu pasal ”ditinggalkan” saat pemerintah dan ...</td>\n",
       "      <td>pasal tinggal perintah dewan wakil rakyat sepa...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_account                                              tweet  \\\n",
       "0     @RahmaFathiyyah  Ketua DPR: Draft RUU Cipta Kerja Hanya Dicek A...   \n",
       "1    @Hendria04991579  Jurus kilat ala sistem kebut skripsi layaknya ...   \n",
       "2       @anis34216465  Sudah jatuh tertimpa tangga. Bebas Covid-19 ja...   \n",
       "4        @EkoFirman15  Mahasiswa cerdas . Pelajaran i dulu RUU CIPTA ...   \n",
       "5          @beereciel  tapi kalau topik ruu cipta kerja,,, SEBERAPA B...   \n",
       "..                ...                                                ...   \n",
       "876  @RosidinBrawija4  ampuun dah gmna mo baca RUU cipta kerja yg set...   \n",
       "878    @RizalAriyadi3  RUU Cipta Kerja Putus Rantai Mafia Birokrasi !...   \n",
       "880      @sukmariyati  rangorang di lab pada rame bahas RUU cipta ker...   \n",
       "881        @mamaciaaa  Jadi, naskah yg dirapihkan terkait kesalahan p...   \n",
       "882     @hariankompas  Satu pasal ”ditinggalkan” saat pemerintah dan ...   \n",
       "\n",
       "                                           clean_tweet  sentimen   ruu  \n",
       "0    ketua dewan wakil rakyat draf rancang undang u...         0  True  \n",
       "1    jurus kilat ala sistem kebut skripsi layak mah...         0  True  \n",
       "2    jatuh timpa tangga bebas covid panggang api pa...         0  True  \n",
       "4    mahasiswa cerdas ajar rancang undang undang ci...         1  True  \n",
       "5       topik rancang undang undang cipta kerja mampus         0  True  \n",
       "..                                                 ...       ...   ...  \n",
       "876  ampun baca rancang undang undang cipta kerja t...         0  True  \n",
       "878  rancang undang undang cipta kerja putus rantai...         1  True  \n",
       "880  orang orang lab ramai bahas rancang undang und...         1  True  \n",
       "881  naskah rapi kait salah tulis salah ketik tanda...         1  True  \n",
       "882  pasal tinggal perintah dewan wakil rakyat sepa...         0  True  \n",
       "\n",
       "[782 rows x 5 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.drop_duplicates(subset=\"tweet_clean\", keep='first', inplace=True)\n",
    "# dataset.to_csv('news_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    analisis = TextBlob(text)\n",
    "    if analisis.sentiment.polarity >= 0.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data1 = pd.read_csv('bhs_data_1_2000.csv')\n",
    "raw_data2 = pd.read_csv('bhs_data_2001_4000.csv')\n",
    "raw_data3 = pd.read_csv('bhs_data_4001_6000.csv')\n",
    "raw_data4 = pd.read_csv('bhs_data_6001_7629.csv')\n",
    "\n",
    "df1 = pd.DataFrame(raw_data1[['Handle', 'Text', 'tweet_clean','bhs_ing']])\n",
    "df2 = pd.DataFrame(raw_data2[['Handle', 'Text', 'tweet_clean','bhs_ing']])\n",
    "df3 = pd.DataFrame(raw_data3[['Handle', 'Text', 'tweet_clean','bhs_ing']])\n",
    "df4 = pd.DataFrame(raw_data4[['Handle', 'Text', 'tweet_clean','bhs_ing']])\n",
    "\n",
    "df_1 = pd.merge(df1,df2, how='outer')\n",
    "df_2 = pd.merge(df3,df4, how='outer')\n",
    "df = pd.merge(df_1,df_2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['bhs_ing'].apply(lambda tweet: sentiment(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.to_csv('sentiment_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('new_dataset.csv')\n",
    "df = pd.DataFrame(raw_data[['Handle', 'Text', 'tweet_clean']])\n",
    "df['sentiment'] = df['tweet_clean'].apply(\n",
    "    lambda tweet: sentiment(tweet))\n",
    "df.to_csv('sentiment_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    translator = Translator()\n",
    "    lang_en = translator.translate(text, src='id', dest='en')\n",
    "    return lang_en.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['bhs_inggris'] = dataset['tweet_clean'].apply(lambda tweet: translate(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('new_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    analisis = TextBlob(text)\n",
    "    an = analisis.translate(from_lang='id',to='en')\n",
    "    if an.sentiment.polarity >= 0.0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.drop(['remove_user', 'remove_symbol', 'remove_duplicate_char', 'remove_emojis', 'tweet_clean', 'ruu'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_csv('sentimen_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfFeature(object):\n",
    " \n",
    "    def __init__(self):\n",
    "        self.tf_dict = {}\n",
    "        self.idf_dict = {}\n",
    "        \n",
    "\n",
    "    def __tokenize(self, tweet):\n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        return tokenizer.tokenize(tweet)\n",
    "\n",
    "    def __calc_TF_Dict(self, document):\n",
    "        TF_dict = {}\n",
    "        for term in document:\n",
    "            if term in TF_dict:\n",
    "                TF_dict[term] += 1\n",
    "            else:\n",
    "                TF_dict[term] = 1\n",
    "        return TF_dict\n",
    "\n",
    "    def __calc_count_Dict(self, tfDict):\n",
    "        count_DF = {}\n",
    "        for document in tfDict:\n",
    "            for term in document:\n",
    "                if term in count_DF:\n",
    "                    count_DF[term] += 1\n",
    "                else:\n",
    "                    count_DF[term] = 1\n",
    "        return count_DF\n",
    "\n",
    "    def __calc_IDF_Dict(self, __n_document, __DF):\n",
    "        IDF_Dict = {}\n",
    "        for term in __DF:\n",
    "            IDF_Dict[term] = np.log(__n_document / __DF[term])\n",
    "        return IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF(self, TF):\n",
    "        TF_IDF_Dict = {}\n",
    "        for key in TF:\n",
    "            TF_IDF_Dict[key] = self.tf_dict[key] * self.idf_dict[key]\n",
    "        return TF_IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF_Vec(self, __TF_IDF_Dict):\n",
    "        wordDict = sorted(self.tf_dict.keys())\n",
    "        TF_IDF_vector = [0.0] * len(wordDict)\n",
    "\n",
    "        for i, term in enumerate(wordDict):\n",
    "            if term in __TF_IDF_Dict:\n",
    "                TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "        return TF_IDF_vector\n",
    "\n",
    "    def set_tf_idf_dict(self, data):\n",
    "        data['tweet_token'] = data['tweet_clean'].apply(self.__tokenize)\n",
    "        data[\"tf_dict\"] = data['tweet_token'].apply(self.__calc_TF_Dict)\n",
    "        self.tf_dict = self.__calc_count_Dict(data[\"tf_dict\"])\n",
    "        self.idf_dict = self.__calc_IDF_Dict(len(data),  self.tf_dict)\n",
    "\n",
    "    def calc_tf_idf(self, data):\n",
    "        data_token = data.apply(self.__tokenize)\n",
    "        data_tf_dict = data_token.apply(self.__calc_TF_Dict)\n",
    "        data_tfidf_dict = data_tf_dict.apply(self.__calc_TF_IDF)\n",
    "        tfidf_vector = [self.__calc_TF_IDF_Vec(row) for row in data_tfidf_dict]\n",
    "        return tfidf_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = TfidfFeature()\n",
    "feature.set_tf_idf_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['tweet_clean'], dataset['sentiment'], test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train = feature.calc_tf_idf(x_train)\n",
    "ft_test = feature.calc_tf_idf(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _predict(self, x_test):\n",
    "        # Calculate posterior for each class\n",
    "        posteriors = []\n",
    "        for idx, _ in enumerate(self._classes):\n",
    "            prior_c = np.log(self._priors[idx])\n",
    "            conditionals_c = self._calc_conditionals(\n",
    "                self._conditionals[idx, :], x_test)\n",
    "            posteriors_c = np.sum(conditionals_c) + prior_c\n",
    "            posteriors.append(posteriors_c)\n",
    "\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _calc_conditionals(self, cls_cond, x_test):\n",
    "        return np.log(cls_cond) * x_test\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        m, n = X_train.shape\n",
    "        self._classes = np.unique(y_train)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # init: Prior & Conditional\n",
    "        self._priors = np.zeros(n_classes)\n",
    "        self._conditionals = np.zeros((n_classes, n))\n",
    "\n",
    "        # Get Prior and Conditional\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_train_c = X_train[c == y_train]\n",
    "            self._priors[idx] = X_train_c.shape[0] / m\n",
    "            self._conditionals[idx, :] = ((X_train_c.sum(axis=0)) + self.alpha) / (np.sum(X_train_c.sum(axis=0) + self.alpha))\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return [self._predict(x_test) for x_test in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(ft_train, y_train)\n",
    "predict = nb.predict(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confus = confusion_matrix(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confus.ravel()\n",
    "accuracy = ((tp + tn)/(tp + tn + fp + fn))*100\n",
    "precision = (tp / (tp + fp))*100\n",
    "recall = (tp / (tp + fn))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy =', accuracy)\n",
    "print('precision =', precision)\n",
    "print('recall =', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('sentimen_dataset.csv')\n",
    "df = pd.DataFrame(raw_data1[['Handle', 'Text', 'tweet_clean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def berita(akun):\n",
    "    if akun == '@media_maju':\n",
    "        return True\n",
    "    elif akun == '@SINDOnews':\n",
    "        return True\n",
    "    elif akun == '@Yahoo_ID':\n",
    "        return True\n",
    "    elif akun == '@CNNIDdaily':\n",
    "        return True\n",
    "    elif akun == '@hariankompas':\n",
    "        return True\n",
    "    elif akun == '@MAJALAH_GATRA':\n",
    "        return True\n",
    "    elif akun == '@kompasiana':\n",
    "        return True\n",
    "    elif akun == '@tvOneNews':\n",
    "        return True\n",
    "    elif akun == '@KompasData':\n",
    "        return True\n",
    "    elif akun == '@kompascom':\n",
    "        return True\n",
    "    elif akun == '@BORNEONEWS':\n",
    "        return True\n",
    "    elif akun == '@jpnncom':\n",
    "        return True\n",
    "    elif akun == '@jawapos':\n",
    "        return True\n",
    "    elif akun == '@KalbarOnline':\n",
    "        return True\n",
    "    elif akun == '@SPN_OR_ID':\n",
    "        return True\n",
    "    elif akun == '@detikcom':\n",
    "        return True\n",
    "    elif akun == '@detikinet':\n",
    "        return True\n",
    "    elif akun == '@CNNIndonesia':\n",
    "        return True\n",
    "    elif akun == '@merdekadotcom':\n",
    "        return True\n",
    "    elif akun == '@tvOneNews':\n",
    "        return True\n",
    "    elif akun == '@antaranews':\n",
    "        return True\n",
    "    elif akun == '@Beritasatu':\n",
    "        return True\n",
    "    elif akun == '@cnbcindonesia':\n",
    "        return True\n",
    "    elif akun == '@liputan6dotcom':\n",
    "        return True\n",
    "    elif akun == '@okezonenews':\n",
    "        return True\n",
    "    elif akun == '@SINDOnews':\n",
    "        return True\n",
    "    elif akun == '@suaradotcom':\n",
    "        return True\n",
    "    elif akun == '@tempodotco':\n",
    "        return True\n",
    "    elif akun == '@tribunnews':\n",
    "        return True\n",
    "    elif akun == '@kumparan':\n",
    "        return True\n",
    "    elif akun == '@VIVAcoid':\n",
    "        return True\n",
    "    elif akun == '@republikaonline':\n",
    "        return True\n",
    "    elif akun == '@CNNIndonesia':\n",
    "        return True\n",
    "    elif akun == '@SonoraFM92':\n",
    "        return True\n",
    "    elif akun == '@officialliputan':\n",
    "        return True\n",
    "    elif akun == '@detikfinance':\n",
    "        return True\n",
    "    elif akun == '@tribunkaltim':\n",
    "        return True\n",
    "    elif akun == '@tribunnews':\n",
    "        return True\n",
    "    elif akun == '@maiwanews':\n",
    "        return True\n",
    "    elif akun == '@sbsinews':\n",
    "        return True\n",
    "    elif akun == '@tribunmedan':\n",
    "        return True\n",
    "    elif akun == '@Metro_TV':\n",
    "        return True\n",
    "    elif akun == '@mediablitar':\n",
    "        return True\n",
    "    elif akun == '@MoBMaB13':\n",
    "        return True\n",
    "    elif akun == '@Zuolinjie_TH':\n",
    "        return True\n",
    "    elif akun == '@inyourznx':\n",
    "        return True\n",
    "    elif akun == '@jaemin813_th':\n",
    "        return True\n",
    "    elif akun == '@cloudypolus':\n",
    "        return True\n",
    "    elif akun == '@m_alfqr':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['berita'] = df['user_account'].apply(lambda x: berita(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop('berita', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    if row['berita'] == True:\n",
    "        df = df.drop(i)\n",
    "    \n",
    "\n",
    "df.drop('berita', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('ruu', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('export.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('export_dataset.csv')\n",
    "df = pd.DataFrame(raw[['user_account', 'tweet', 'clean_tweet', 'sentimen']])\n",
    "df['ruu'] = df['clean_tweet'].str.contains('rancang undang undang')\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row['ruu'] == False:\n",
    "        df = df.drop(i)\n",
    "\n",
    "df.drop('ruu', axis=1, inplace=True)\n",
    "df['berita'] = df['user_account'].apply(lambda x: berita(x))\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row['berita'] == True:\n",
    "        df = df.drop(i)\n",
    "    \n",
    "\n",
    "df.drop('berita', axis=1, inplace=True)\n",
    "df.to_csv('export.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('filter_dataset.csv')\n",
    "df = pd.DataFrame(raw[['user_account', 'tweet', 'label']])\n",
    "df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('filter_dataset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
