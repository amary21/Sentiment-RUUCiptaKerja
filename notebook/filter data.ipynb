{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_indonesia = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(object):\n",
    "    def __init__(self):\n",
    "        self.factory = StemmerFactory()\n",
    "        self.stemmer = self.factory.create_stemmer()\n",
    "        self.kamus = self.__get_dictionary()\n",
    "\n",
    "    def __get_dictionary(self):\n",
    "        df = pd.read_csv('normalisasi.csv', sep=';')\n",
    "        dictlist = []\n",
    "        for row in df.values:\n",
    "            dictlist.append([row[0], row[1]])\n",
    "        return dictlist\n",
    "\n",
    "    def __remove_pattern(self, tweet: str, pattern):\n",
    "        r = re.findall(pattern, tweet)\n",
    "        for i in r:\n",
    "            tweet = re.sub(i, '', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def __remove_symbol(self, tweet: str):\n",
    "        tweet = self.__remove_url(tweet)\n",
    "        # get only alfabet\n",
    "        pattern = re.compile(r'\\b[^\\d\\W]+\\b')\n",
    "        newwords = []\n",
    "        for word in pattern.findall(tweet):\n",
    "            # case folding\n",
    "\n",
    "            word = word.lower()\n",
    "            for row in self.kamus:\n",
    "                key = row[0]\n",
    "                value = row[1]\n",
    "                if word == key:\n",
    "                    word = value\n",
    "                    break\n",
    "\n",
    "            word = word.replace(\"xyz\", \"\")\n",
    "            newwords.append(word)\n",
    "        return \" \".join(newwords)\n",
    "\n",
    "    def __remove_url(self, text):\n",
    "        # Remove additional white spaces\n",
    "        text = re.sub('[\\s]+', ' ', text)\n",
    "        text = re.sub('[\\n]+', ' ', text)\n",
    "        # remove all url\n",
    "        text = re.sub(r\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", text)\n",
    "        # remove email\n",
    "        text = re.sub(r\"[\\w]+@[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # remove text twit\n",
    "        text = re.sub(r'((pic\\.[^\\s]+)|(twitter))', '', text)\n",
    "        # remove mentions, hashtag and web\n",
    "        text = re.sub(r\"(?:\\@|#|http?\\://)\\S+\", \"\", text)\n",
    "        # remove url\n",
    "        text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r'((https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r\"(pic[^\\s]+)|[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # replace non ascii\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __remove_emojis(self, data):\n",
    "        emoj = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                          u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                          u\"\\U00002702-\\U000027B0\"  \n",
    "                          u\"\\U000024C2-\\U0001F251\"\n",
    "                          u\"\\U0001f926-\\U0001f937\"\n",
    "                          u\"\\U00010000-\\U0010ffff\"\n",
    "                          u\"\\u2640-\\u2642\"\n",
    "                          u\"\\u2600-\\u2B55\"\n",
    "                          u\"\\u200d\"\n",
    "                          u\"\\u23cf\"\n",
    "                          u\"\\u23e9\"\n",
    "                          u\"\\u231a\"\n",
    "                          u\"\\ufe0f\"  # dingbats\n",
    "                          u\"\\u3030\"\n",
    "                          \"]+\", re.UNICODE)\n",
    "        return re.sub(emoj, '', data)\n",
    "    \n",
    "    def __concate_duplicate(self, tweet):\n",
    "        term = \"a\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", tweet)\n",
    "        term = \"i\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"u\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"e\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"o\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        term = \"c\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"k\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"w\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"h\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        return rep\n",
    "\n",
    "    def __clean_tweets(self, tweet: str) -> str:\n",
    "        # tokenize tweets\n",
    "        \n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        \n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        \n",
    "        tweets_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in stopwords_indonesia and  # remove stopwords\n",
    "                    word not in string.punctuation):  # remove punctuation\n",
    "                tweets_clean.append(word)\n",
    "\n",
    "        stem_word = self.stemmer.stem(\" \".join(tweets_clean))  # stemming word\n",
    "        return stem_word\n",
    "\n",
    "    def from_csv(self, file_name):\n",
    "        raw_data = pd.read_csv(file_name)\n",
    "#         df = pd.DataFrame(raw_data[['user_account', 'tweet', 'label']])\n",
    "        df = pd.DataFrame(raw_data[['Handle', 'Text','Emojis','Comments','Likes','Retweets']])\n",
    "\n",
    "        df['remove_user'] = np.vectorize(self.__remove_pattern)(df['Text'], \"(@\\\\w*)\")\n",
    "        df['remove_symbol'] = df[\"remove_user\"].apply(lambda x: np.vectorize(self.__remove_pattern)(x, \"(#\\\\w*)\"))\n",
    "        df['remove_duplicate_char'] = df['remove_symbol'].apply(self.__concate_duplicate)\n",
    "        df['remove_emojis'] = df['remove_duplicate_char'].apply(lambda x: self.__remove_emojis(self.__remove_symbol(x)))\n",
    "        \n",
    "        df.drop_duplicates(subset=\"remove_emojis\", keep='first', inplace=True)\n",
    "        \n",
    "        df['tweet_clean'] = df['remove_emojis'].apply(lambda x: self.__clean_tweets(x))\n",
    "        df = df.dropna(subset=[\"tweet_clean\"])\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            if row['tweet_clean'] == \"\":\n",
    "                df = df.drop(i)\n",
    "        \n",
    "        df['ruu'] = df['tweet_clean'].str.contains('rancang undang undang')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['ruu'] == False:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['ruu'], axis=1, inplace=True)\n",
    "        \n",
    "        df['thai_account'] = df['Text'].str.contains('#WhatsHappeninglnThailand')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['thai_account'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['thai_account'], axis=1, inplace=True)\n",
    "        \n",
    "        df['phil_account'] = df['Text'].str.contains('#WhatsHappeningInPhilippines')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['phil_account'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['phil_account'], axis=1, inplace=True)\n",
    "        \n",
    "        df['other_account'] = df['Text'].str.contains('#WhatIsHappeningInIndonesia')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['other_account'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['other_account'], axis=1, inplace=True)\n",
    "        \n",
    "        df['other_account'] = df['Text'].str.contains('#WhatsHappeningInIndonesia')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['other_account'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['other_account'], axis=1, inplace=True)\n",
    "        \n",
    "        df['other_tag'] = df['Text'].str.contains('#JunkTerrorBill')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['other_tag'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['other_tag'], axis=1, inplace=True)\n",
    "        \n",
    "        df['other_tag'] = df['Text'].str.contains('Junk Terror Bill')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['other_tag'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['other_tag'], axis=1, inplace=True)\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            if pd.isna(row['Retweets']) or pd.isna(row['Likes']):\n",
    "                df = df.drop(i)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfFeature(object):\n",
    " \n",
    "    def __init__(self):\n",
    "        self.tf_dict = {}\n",
    "        self.idf_dict = {}\n",
    "        \n",
    "\n",
    "    def __tokenize(self, tweet):\n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        return tokenizer.tokenize(tweet)\n",
    "\n",
    "    def __calc_TF_Dict(self, document):\n",
    "        TF_dict = {}\n",
    "        for term in document:\n",
    "            if term in TF_dict:\n",
    "                TF_dict[term] += 1\n",
    "            else:\n",
    "                TF_dict[term] = 1\n",
    "        return TF_dict\n",
    "\n",
    "    def __calc_count_Dict(self, tfDict):\n",
    "        count_DF = {}\n",
    "        for document in tfDict:\n",
    "            for term in document:\n",
    "                if term in count_DF:\n",
    "                    count_DF[term] += 1\n",
    "                else:\n",
    "                    count_DF[term] = 1\n",
    "        return count_DF\n",
    "\n",
    "    def __calc_IDF_Dict(self, __n_document, __DF):\n",
    "        IDF_Dict = {}\n",
    "        for term in __DF:\n",
    "            IDF_Dict[term] = np.log(__n_document / __DF[term])\n",
    "        return IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF(self, TF):\n",
    "        TF_IDF_Dict = {}\n",
    "        for key in TF:\n",
    "            TF_IDF_Dict[key] = self.tf_dict[key] * self.idf_dict[key]\n",
    "        return TF_IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF_Vec(self, __TF_IDF_Dict):\n",
    "        wordDict = sorted(self.tf_dict.keys())\n",
    "        TF_IDF_vector = [0.0] * len(wordDict)\n",
    "\n",
    "        for i, term in enumerate(wordDict):\n",
    "            if term in __TF_IDF_Dict:\n",
    "                TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "        return TF_IDF_vector\n",
    "\n",
    "    def set_tf_idf_dict(self, data):\n",
    "        data['tweet_token'] = data['tweet_clean'].apply(self.__tokenize)\n",
    "        data[\"tf_dict\"] = data['tweet_token'].apply(self.__calc_TF_Dict)\n",
    "        self.tf_dict = self.__calc_count_Dict(data[\"tf_dict\"])\n",
    "        self.idf_dict = self.__calc_IDF_Dict(len(data),  self.tf_dict)\n",
    "\n",
    "    def calc_tf_idf(self, data):\n",
    "        data_token = data.apply(self.__tokenize)\n",
    "        data_tf_dict = data_token.apply(self.__calc_TF_Dict)\n",
    "        data_tfidf_dict = data_tf_dict.apply(self.__calc_TF_IDF)\n",
    "        tfidf_vector = [self.__calc_TF_IDF_Vec(row) for row in data_tfidf_dict]\n",
    "        return tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _predict(self, x_test):\n",
    "        # Calculate posterior for each class\n",
    "        posteriors = []\n",
    "        for idx, _ in enumerate(self._classes):\n",
    "            prior_c = np.log10(self._priors[idx])\n",
    "            conditionals_c = self._calc_conditionals(\n",
    "                self._conditionals[idx, :], x_test)\n",
    "            posteriors_c = np.sum(conditionals_c) + prior_c\n",
    "            posteriors.append(posteriors_c)\n",
    "\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _calc_conditionals(self, cls_cond, x_test):\n",
    "        return np.log(cls_cond) * x_test\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        m, n = X_train.shape\n",
    "        self._classes = np.unique(y_train)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # init: Prior & Conditional\n",
    "        self._priors = np.zeros(n_classes)\n",
    "        self._conditionals = np.zeros((n_classes, n))\n",
    "\n",
    "        # Get Prior and Conditional\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_train_c = X_train[c == y_train]\n",
    "            self._priors[idx] = X_train_c.shape[0] / m\n",
    "            self._conditionals[idx, :] = ((X_train_c.sum(axis=0)) + self.alpha) / (np.sum(X_train_c.sum(axis=0) + self.alpha))\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return [self._predict(x_test) for x_test in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def performance(y_test, y_predict):\n",
    "    confus = confusion_matrix(y_test, predict)\n",
    "    tn, fp, fn, tp = confus.ravel()\n",
    "    accuracy = ((tp + tn)/(tp + tn + fp + fn))*100\n",
    "    precision = (tp / (tp + fp))*100\n",
    "    recall = (tp / (tp + fn))*100\n",
    "    print('accuracy =', accuracy)\n",
    "    print('precision =', precision)\n",
    "    print('recall =', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Handle</th>\n",
       "      <th>Text</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>remove_user</th>\n",
       "      <th>remove_symbol</th>\n",
       "      <th>remove_duplicate_char</th>\n",
       "      <th>remove_emojis</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@KRuHA_Indonesia</td>\n",
       "      <td>“ Tak Ada Rakyat Dalam Omnibus Law/RUU Cipta K...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>“ Tak Ada Rakyat Dalam Omnibus Law/RUU Cipta K...</td>\n",
       "      <td>“ Tak Ada Rakyat Dalam Omnibus Law/RUU Cipta K...</td>\n",
       "      <td>“ Tak Ada Rakyat Dalam Omnibus Law/RUU Cipta K...</td>\n",
       "      <td>tak ada rakyat dalam omnibus law rancangan und...</td>\n",
       "      <td>rakyat omnibus law rancang undang undang cipta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@sekar05vita</td>\n",
       "      <td>Kena macet karena demo tolak RUU Cipta Kerja. ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Kena macet karena demo tolak RUU Cipta Kerja. ...</td>\n",
       "      <td>Kena macet karena demo tolak RUU Cipta Kerja. ...</td>\n",
       "      <td>Kena macet karena demo tolak RUU Cipta Kerja. ...</td>\n",
       "      <td>kena macet karena demo tolak rancangan undang ...</td>\n",
       "      <td>kena macet demo tolak rancang undang undang ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@MalvaSelena</td>\n",
       "      <td>#6TahunJokowi \\n Pembahasan penyusunan Rancang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>#6TahunJokowi \\n Pembahasan penyusunan Rancang...</td>\n",
       "      <td>\\n Pembahasan penyusunan Rancangan Undang-und...</td>\n",
       "      <td>\\n Pembahasan penyusunan Rancangan Undang-und...</td>\n",
       "      <td>pembahasan penyusunan rancangan undang undang ...</td>\n",
       "      <td>bahas susun rancang undang undang rancang unda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>@TIARA2796</td>\n",
       "      <td>REDISTRIBUSI TANAH, REVOLUSI DARI JOKOWI! \\n\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>REDISTRIBUSI TANAH, REVOLUSI DARI JOKOWI! \\n\\n...</td>\n",
       "      <td>REDISTRIBUSI TANAH, REVOLUSI DARI JOKOWI! \\n\\n...</td>\n",
       "      <td>REDISTRIBUSI TANAH, REVOLUSI DARI JOKOWI! \\n\\n...</td>\n",
       "      <td>redistribusi tanah revolusi dari jokowi ada se...</td>\n",
       "      <td>redistribusi tanah revolusi jokowi tarik konse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>@Paltiwest</td>\n",
       "      <td>#6TahunJokowi yuk disimak isu-isu penting dala...</td>\n",
       "      <td>☕</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>#6TahunJokowi yuk disimak isu-isu penting dala...</td>\n",
       "      <td>yuk disimak isu-isu penting dalam UU Cipta Ke...</td>\n",
       "      <td>yuk disimak isu-isu penting dalam UU Cipta Ke...</td>\n",
       "      <td>ayo disimak isu isu penting dalam undang undan...</td>\n",
       "      <td>ayo simak isu isu undang undang cipta kerja gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15044</th>\n",
       "      <td>@RadioElshinta</td>\n",
       "      <td>Yuk, simak poin-poin yang diatur dalam klaster...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>Yuk, simak poin-poin yang diatur dalam klaster...</td>\n",
       "      <td>Yuk, simak poin-poin yang diatur dalam klaster...</td>\n",
       "      <td>Yuk, simak poin-poin yang diatur dalam klaster...</td>\n",
       "      <td>ayo simak poin poin yang diatur dalam kluster ...</td>\n",
       "      <td>ayo simak poin poin atur kluster ketenagakerja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15056</th>\n",
       "      <td>@Sijeuni_23</td>\n",
       "      <td>Membalas \\n@chaeRyboomMmm cuma mau bilang, has...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>Membalas \\n cuma mau bilang, hastag nya yg sed...</td>\n",
       "      <td>Membalas \\n cuma mau bilang, hastag nya yg sed...</td>\n",
       "      <td>Membalas \\n cuma mau bilang, hastag nya yg sed...</td>\n",
       "      <td>membalas cuma mau bilang hashtag  yang sedikit...</td>\n",
       "      <td>balas bilang hashtag biar cepat percaya deh ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15074</th>\n",
       "      <td>@MWuryantati</td>\n",
       "      <td>Pada Senin, 5 Oktober 2020 kemarin dalam Parip...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Pada Senin, 5 Oktober 2020 kemarin dalam Parip...</td>\n",
       "      <td>Pada Senin, 5 Oktober 2020 kemarin dalam Parip...</td>\n",
       "      <td>Pada Senin, 5 Oktober 2020 kemarin dalam Parip...</td>\n",
       "      <td>pada senin oktober kemarin dalam paripurna dew...</td>\n",
       "      <td>senin oktober kemarin paripurna dewan wakil ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15102</th>\n",
       "      <td>@MWuryantati</td>\n",
       "      <td>Berbagai usulan masyarakat spt angin lalu, mes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Berbagai usulan masyarakat spt angin lalu, mes...</td>\n",
       "      <td>Berbagai usulan masyarakat spt angin lalu, mes...</td>\n",
       "      <td>Berbagai usulan masyarakat spt angin lalu, mes...</td>\n",
       "      <td>berbagai usulan masyarakat seperti angin lalu ...</td>\n",
       "      <td>usul masyarakat angin argumentasi filosofis ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15170</th>\n",
       "      <td>@duniazie</td>\n",
       "      <td>Jadi.. sejak digagas oleh Menko Perekonomian, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jadi.. sejak digagas oleh Menko Perekonomian, ...</td>\n",
       "      <td>Jadi.. sejak digagas oleh Menko Perekonomian, ...</td>\n",
       "      <td>Jadi.. sejak digagas oleh Menko Perekonomian, ...</td>\n",
       "      <td>jadi sejak digagas oleh menteri koordinator pe...</td>\n",
       "      <td>gagas menteri koordinator ekonomi rancang unda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1781 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Handle                                               Text  \\\n",
       "11     @KRuHA_Indonesia  “ Tak Ada Rakyat Dalam Omnibus Law/RUU Cipta K...   \n",
       "12         @sekar05vita  Kena macet karena demo tolak RUU Cipta Kerja. ...   \n",
       "15         @MalvaSelena  #6TahunJokowi \\n Pembahasan penyusunan Rancang...   \n",
       "33           @TIARA2796  REDISTRIBUSI TANAH, REVOLUSI DARI JOKOWI! \\n\\n...   \n",
       "34           @Paltiwest  #6TahunJokowi yuk disimak isu-isu penting dala...   \n",
       "...                 ...                                                ...   \n",
       "15044    @RadioElshinta  Yuk, simak poin-poin yang diatur dalam klaster...   \n",
       "15056       @Sijeuni_23  Membalas \\n@chaeRyboomMmm cuma mau bilang, has...   \n",
       "15074      @MWuryantati  Pada Senin, 5 Oktober 2020 kemarin dalam Parip...   \n",
       "15102      @MWuryantati  Berbagai usulan masyarakat spt angin lalu, mes...   \n",
       "15170         @duniazie  Jadi.. sejak digagas oleh Menko Perekonomian, ...   \n",
       "\n",
       "      Emojis  Comments Likes Retweets  \\\n",
       "11       NaN       NaN     1        1   \n",
       "12       NaN       NaN     1        4   \n",
       "15       NaN       1.0     7        6   \n",
       "33       NaN       1.0     3        4   \n",
       "34         ☕       NaN     8        8   \n",
       "...      ...       ...   ...      ...   \n",
       "15044    NaN       3.0    21       18   \n",
       "15056    NaN       1.0    16       13   \n",
       "15074    NaN       NaN     1        5   \n",
       "15102    NaN       NaN     1        6   \n",
       "15170    NaN       NaN     1        1   \n",
       "\n",
       "                                             remove_user  \\\n",
       "11     “ Tak Ada Rakyat Dalam Omnibus Law/RUU Cipta K...   \n",
       "12     Kena macet karena demo tolak RUU Cipta Kerja. ...   \n",
       "15     #6TahunJokowi \\n Pembahasan penyusunan Rancang...   \n",
       "33     REDISTRIBUSI TANAH, REVOLUSI DARI JOKOWI! \\n\\n...   \n",
       "34     #6TahunJokowi yuk disimak isu-isu penting dala...   \n",
       "...                                                  ...   \n",
       "15044  Yuk, simak poin-poin yang diatur dalam klaster...   \n",
       "15056  Membalas \\n cuma mau bilang, hastag nya yg sed...   \n",
       "15074  Pada Senin, 5 Oktober 2020 kemarin dalam Parip...   \n",
       "15102  Berbagai usulan masyarakat spt angin lalu, mes...   \n",
       "15170  Jadi.. sejak digagas oleh Menko Perekonomian, ...   \n",
       "\n",
       "                                           remove_symbol  \\\n",
       "11     “ Tak Ada Rakyat Dalam Omnibus Law/RUU Cipta K...   \n",
       "12     Kena macet karena demo tolak RUU Cipta Kerja. ...   \n",
       "15      \\n Pembahasan penyusunan Rancangan Undang-und...   \n",
       "33     REDISTRIBUSI TANAH, REVOLUSI DARI JOKOWI! \\n\\n...   \n",
       "34      yuk disimak isu-isu penting dalam UU Cipta Ke...   \n",
       "...                                                  ...   \n",
       "15044  Yuk, simak poin-poin yang diatur dalam klaster...   \n",
       "15056  Membalas \\n cuma mau bilang, hastag nya yg sed...   \n",
       "15074  Pada Senin, 5 Oktober 2020 kemarin dalam Parip...   \n",
       "15102  Berbagai usulan masyarakat spt angin lalu, mes...   \n",
       "15170  Jadi.. sejak digagas oleh Menko Perekonomian, ...   \n",
       "\n",
       "                                   remove_duplicate_char  \\\n",
       "11     “ Tak Ada Rakyat Dalam Omnibus Law/RUU Cipta K...   \n",
       "12     Kena macet karena demo tolak RUU Cipta Kerja. ...   \n",
       "15      \\n Pembahasan penyusunan Rancangan Undang-und...   \n",
       "33     REDISTRIBUSI TANAH, REVOLUSI DARI JOKOWI! \\n\\n...   \n",
       "34      yuk disimak isu-isu penting dalam UU Cipta Ke...   \n",
       "...                                                  ...   \n",
       "15044  Yuk, simak poin-poin yang diatur dalam klaster...   \n",
       "15056  Membalas \\n cuma mau bilang, hastag nya yg sed...   \n",
       "15074  Pada Senin, 5 Oktober 2020 kemarin dalam Parip...   \n",
       "15102  Berbagai usulan masyarakat spt angin lalu, mes...   \n",
       "15170  Jadi.. sejak digagas oleh Menko Perekonomian, ...   \n",
       "\n",
       "                                           remove_emojis  \\\n",
       "11     tak ada rakyat dalam omnibus law rancangan und...   \n",
       "12     kena macet karena demo tolak rancangan undang ...   \n",
       "15     pembahasan penyusunan rancangan undang undang ...   \n",
       "33     redistribusi tanah revolusi dari jokowi ada se...   \n",
       "34     ayo disimak isu isu penting dalam undang undan...   \n",
       "...                                                  ...   \n",
       "15044  ayo simak poin poin yang diatur dalam kluster ...   \n",
       "15056  membalas cuma mau bilang hashtag  yang sedikit...   \n",
       "15074  pada senin oktober kemarin dalam paripurna dew...   \n",
       "15102  berbagai usulan masyarakat seperti angin lalu ...   \n",
       "15170  jadi sejak digagas oleh menteri koordinator pe...   \n",
       "\n",
       "                                             tweet_clean  \n",
       "11     rakyat omnibus law rancang undang undang cipta...  \n",
       "12     kena macet demo tolak rancang undang undang ci...  \n",
       "15     bahas susun rancang undang undang rancang unda...  \n",
       "33     redistribusi tanah revolusi jokowi tarik konse...  \n",
       "34     ayo simak isu isu undang undang cipta kerja gr...  \n",
       "...                                                  ...  \n",
       "15044  ayo simak poin poin atur kluster ketenagakerja...  \n",
       "15056  balas bilang hashtag biar cepat percaya deh ce...  \n",
       "15074  senin oktober kemarin paripurna dewan wakil ra...  \n",
       "15102  usul masyarakat angin argumentasi filosofis ju...  \n",
       "15170  gagas menteri koordinator ekonomi rancang unda...  \n",
       "\n",
       "[1781 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = 'ruu_cipta_kerja_20102020_terbaru.csv'\n",
    "preprocessing = Preprocessing()\n",
    "df = preprocessing.from_csv(raw_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['thai_account'] = df['remove_user'].str.contains('#WhatsHappeninglnThailand')\n",
    "df\n",
    "# for i, row in df.iterrows():\n",
    "#     if row['thai_account']:\n",
    "#         df = df.drop(i)\n",
    "# df.drop(['thai_account'], axis=1, inplace=True)\n",
    "# df\n",
    "\n",
    "# df['phil_account'] = df['Text'].str.contains('#WhatsHappeningInPhilippines')\n",
    "# for i, row in df.iterrows():\n",
    "#     if row['phil_account'] == True:\n",
    "#         df = df.drop(i)\n",
    "# df.drop(['phil_account'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df\n",
    "for i, row in cdf.iterrows():\n",
    "#     print(pd.isna(row['Retweets']))\n",
    "    if pd.isna(row['Retweets']) or pd.isna(row['Likes']):\n",
    "        cdf = cdf.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_09062021_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cdf)\n",
    "cdf.to_csv('df_09062021.csv')\n",
    "# feature = TfidfFeature()\n",
    "# feature.set_tf_idf_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_export():\n",
    "    raw = pd.read_csv('drive_dataset.csv')\n",
    "    df = pd.DataFrame(raw[['tweet','label']])\n",
    "    dictlist = []\n",
    "    for row in df.values:\n",
    "        dictlist.append([row[0], row[1]])\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_export(tweet):\n",
    "    g_export = get_export()\n",
    "    for row in g_export:\n",
    "        key = row[0]\n",
    "        value = row[1]\n",
    "        if tweet == key:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentimen'] = df['Text'].apply(lambda x: clean_export(x))\n",
    "\n",
    "# for i, row in cdf.iterrows():\n",
    "#     if row['clean_export'] == True:\n",
    "#         cdf = cdf.drop(i)\n",
    "\n",
    "# cdf.drop(['clean_export'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def akun_asing(akun):\n",
    "    if akun == '@MoBMaB13':\n",
    "        return True\n",
    "    elif akun == '@inyourznx':\n",
    "        return True\n",
    "    elif akun == '@jaemin813_th':\n",
    "        return True\n",
    "    elif akun == '@cloudypolus':\n",
    "        return True\n",
    "    elif akun == '@__TakagiBot':\n",
    "        return True\n",
    "    elif akun == '@so_r_u_happynow':\n",
    "        return True\n",
    "    elif akun == '@inyourznx':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf['akun_asing'] = cdf['Handle'].apply(lambda x: akun_asing(x))\n",
    "\n",
    "for i, row in cdf.iterrows():\n",
    "    if row['akun_asing'] == True:\n",
    "        cdf = cdf.drop(i)\n",
    "    \n",
    "\n",
    "cdf.drop('akun_asing', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cdf.to_csv('df_09062021_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df['tweet_clean'], df['label'], test_size=1., shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train = feature.calc_tf_idf(x_train)\n",
    "ft_test = feature.calc_tf_idf(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(ft_train, y_train)\n",
    "predict = nb.predict(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('like_dataset_0.csv')\n",
    "df = pd.DataFrame(raw_data[['user_account', 'label','tweet','tweet_clean']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
