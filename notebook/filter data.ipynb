{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_indonesia = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(object):\n",
    "    def __init__(self):\n",
    "        self.factory = StemmerFactory()\n",
    "        self.stemmer = self.factory.create_stemmer()\n",
    "        self.kamus = self.__get_dictionary()\n",
    "\n",
    "    def __get_dictionary(self):\n",
    "        df = pd.read_csv('normalisasi.csv', sep=';')\n",
    "        dictlist = []\n",
    "        for row in df.values:\n",
    "            dictlist.append([row[0], row[1]])\n",
    "        return dictlist\n",
    "\n",
    "    def __remove_pattern(self, tweet: str, pattern):\n",
    "        r = re.findall(pattern, tweet)\n",
    "        for i in r:\n",
    "            tweet = re.sub(i, '', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def __remove_symbol(self, tweet: str):\n",
    "        tweet = self.__remove_url(tweet)\n",
    "        # get only alfabet\n",
    "        pattern = re.compile(r'\\b[^\\d\\W]+\\b')\n",
    "        newwords = []\n",
    "        for word in pattern.findall(tweet):\n",
    "            # case folding\n",
    "\n",
    "            word = word.lower()\n",
    "            for row in self.kamus:\n",
    "                key = row[0]\n",
    "                value = row[1]\n",
    "                if word == key:\n",
    "                    word = value\n",
    "                    break\n",
    "\n",
    "            word = word.replace(\"xyz\", \"\")\n",
    "            newwords.append(word)\n",
    "        return \" \".join(newwords)\n",
    "\n",
    "    def __remove_url(self, text):\n",
    "        # Remove additional white spaces\n",
    "        text = re.sub('[\\s]+', ' ', text)\n",
    "        text = re.sub('[\\n]+', ' ', text)\n",
    "        # remove all url\n",
    "        text = re.sub(r\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", text)\n",
    "        # remove email\n",
    "        text = re.sub(r\"[\\w]+@[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # remove text twit\n",
    "        text = re.sub(r'((pic\\.[^\\s]+)|(twitter))', '', text)\n",
    "        # remove mentions, hashtag and web\n",
    "        text = re.sub(r\"(?:\\@|#|http?\\://)\\S+\", \"\", text)\n",
    "        # remove url\n",
    "        text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r'((https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r\"(pic[^\\s]+)|[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # replace non ascii\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __remove_emojis(self, data):\n",
    "        emoj = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                          u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                          u\"\\U00002702-\\U000027B0\"  \n",
    "                          u\"\\U000024C2-\\U0001F251\"\n",
    "                          u\"\\U0001f926-\\U0001f937\"\n",
    "                          u\"\\U00010000-\\U0010ffff\"\n",
    "                          u\"\\u2640-\\u2642\"\n",
    "                          u\"\\u2600-\\u2B55\"\n",
    "                          u\"\\u200d\"\n",
    "                          u\"\\u23cf\"\n",
    "                          u\"\\u23e9\"\n",
    "                          u\"\\u231a\"\n",
    "                          u\"\\ufe0f\"  # dingbats\n",
    "                          u\"\\u3030\"\n",
    "                          \"]+\", re.UNICODE)\n",
    "        return re.sub(emoj, '', data)\n",
    "    \n",
    "    def __concate_duplicate(self, tweet):\n",
    "        term = \"a\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", tweet)\n",
    "        term = \"i\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"u\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"e\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"o\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        term = \"c\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"k\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"w\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"h\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        return rep\n",
    "\n",
    "    def __clean_tweets(self, tweet: str) -> str:\n",
    "        # tokenize tweets\n",
    "        \n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        \n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        \n",
    "        tweets_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in stopwords_indonesia and  # remove stopwords\n",
    "                    word not in string.punctuation):  # remove punctuation\n",
    "                tweets_clean.append(word)\n",
    "\n",
    "        stem_word = self.stemmer.stem(\" \".join(tweets_clean))  # stemming word\n",
    "        return stem_word\n",
    "    \n",
    "    def __berita(self, akun):\n",
    "        if akun == '@media_maju':\n",
    "            return True\n",
    "        elif akun == '@SINDOnews':\n",
    "            return True\n",
    "        elif akun == '@Yahoo_ID':\n",
    "            return True\n",
    "        elif akun == '@CNNIDdaily':\n",
    "            return True\n",
    "        elif akun == '@hariankompas':\n",
    "            return True\n",
    "        elif akun == '@MAJALAH_GATRA':\n",
    "            return True\n",
    "        elif akun == '@kompasiana':\n",
    "            return True\n",
    "        elif akun == '@tvOneNews':\n",
    "            return True\n",
    "        elif akun == '@KompasData':\n",
    "            return True\n",
    "        elif akun == '@kompascom':\n",
    "            return True\n",
    "        elif akun == '@BORNEONEWS':\n",
    "            return True\n",
    "        elif akun == '@jpnncom':\n",
    "            return True\n",
    "        elif akun == '@jawapos':\n",
    "            return True\n",
    "        elif akun == '@KalbarOnline':\n",
    "            return True\n",
    "        elif akun == '@SPN_OR_ID':\n",
    "            return True\n",
    "        elif akun == '@detikcom':\n",
    "            return True\n",
    "        elif akun == '@detikinet':\n",
    "            return True\n",
    "        elif akun == '@CNNIndonesia':\n",
    "            return True\n",
    "        elif akun == '@merdekadotcom':\n",
    "            return True\n",
    "        elif akun == '@tvOneNews':\n",
    "            return True\n",
    "        elif akun == '@antaranews':\n",
    "            return True\n",
    "        elif akun == '@Beritasatu':\n",
    "            return True\n",
    "        elif akun == '@cnbcindonesia':\n",
    "            return True\n",
    "        elif akun == '@liputan6dotcom':\n",
    "            return True\n",
    "        elif akun == '@okezonenews':\n",
    "            return True\n",
    "        elif akun == '@SINDOnews':\n",
    "            return True\n",
    "        elif akun == '@suaradotcom':\n",
    "            return True\n",
    "        elif akun == '@tempodotco':\n",
    "            return True\n",
    "        elif akun == '@tribunnews':\n",
    "            return True\n",
    "        elif akun == '@kumparan':\n",
    "            return True\n",
    "        elif akun == '@VIVAcoid':\n",
    "            return True\n",
    "        elif akun == '@republikaonline':\n",
    "            return True\n",
    "        elif akun == '@CNNIndonesia':\n",
    "            return True\n",
    "        elif akun == '@SonoraFM92':\n",
    "            return True\n",
    "        elif akun == '@officialliputan':\n",
    "            return True\n",
    "        elif akun == '@detikfinance':\n",
    "            return True\n",
    "        elif akun == '@tribunkaltim':\n",
    "            return True\n",
    "        elif akun == '@tribunnews':\n",
    "            return True\n",
    "        elif akun == '@maiwanews':\n",
    "            return True\n",
    "        elif akun == '@sbsinews':\n",
    "            return True\n",
    "        elif akun == '@tribunmedan':\n",
    "            return True\n",
    "        elif akun == '@Metro_TV':\n",
    "            return True\n",
    "        elif akun == '@mediablitar':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def from_csv(self, file_name):\n",
    "        raw_data = pd.read_csv(file_name)\n",
    "#         df = pd.DataFrame(raw_data[['user_account', 'tweet', 'label']])\n",
    "        df = pd.DataFrame(raw_data[['user_account', 'tweet','label']])\n",
    "\n",
    "        df['remove_user'] = np.vectorize(self.__remove_pattern)(df['tweet'], \"(@\\\\w*)\")\n",
    "        df['remove_symbol'] = df[\"remove_user\"].apply(lambda x: np.vectorize(self.__remove_pattern)(x, \"(#\\\\w*)\"))\n",
    "        df['remove_duplicate_char'] = df['remove_symbol'].apply(self.__concate_duplicate)\n",
    "        df['remove_emojis'] = df['remove_duplicate_char'].apply(lambda x: self.__remove_emojis(self.__remove_symbol(x)))\n",
    "        \n",
    "        df.drop_duplicates(subset=\"remove_emojis\", keep='first', inplace=True)\n",
    "        \n",
    "        df['tweet_clean'] = df['remove_emojis'].apply(lambda x: self.__clean_tweets(x))\n",
    "        df = df.dropna(subset=[\"tweet_clean\"])\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            if row['tweet_clean'] == \"\":\n",
    "                df = df.drop(i)\n",
    "        \n",
    "        df['ruu'] = df['tweet_clean'].str.contains('rancang undang undang')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['ruu'] == False:\n",
    "                df = df.drop(i)\n",
    "        \n",
    "        df['berita'] = df['user_account'].apply(lambda x: self.__berita(x))\n",
    "        for i, row in df.iterrows():\n",
    "            if row['berita'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['remove_user', 'remove_symbol', 'remove_duplicate_char', 'remove_emojis', 'ruu', 'berita'], axis=1, inplace=True)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfFeature(object):\n",
    " \n",
    "    def __init__(self):\n",
    "        self.tf_dict = {}\n",
    "        self.idf_dict = {}\n",
    "        \n",
    "\n",
    "    def __tokenize(self, tweet):\n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        return tokenizer.tokenize(tweet)\n",
    "\n",
    "    def __calc_TF_Dict(self, document):\n",
    "        TF_dict = {}\n",
    "        for term in document:\n",
    "            if term in TF_dict:\n",
    "                TF_dict[term] += 1\n",
    "            else:\n",
    "                TF_dict[term] = 1\n",
    "        return TF_dict\n",
    "\n",
    "    def __calc_count_Dict(self, tfDict):\n",
    "        count_DF = {}\n",
    "        for document in tfDict:\n",
    "            for term in document:\n",
    "                if term in count_DF:\n",
    "                    count_DF[term] += 1\n",
    "                else:\n",
    "                    count_DF[term] = 1\n",
    "        return count_DF\n",
    "\n",
    "    def __calc_IDF_Dict(self, __n_document, __DF):\n",
    "        IDF_Dict = {}\n",
    "        for term in __DF:\n",
    "            IDF_Dict[term] = np.log(__n_document / __DF[term])\n",
    "        return IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF(self, TF):\n",
    "        TF_IDF_Dict = {}\n",
    "        for key in TF:\n",
    "            TF_IDF_Dict[key] = self.tf_dict[key] * self.idf_dict[key]\n",
    "        return TF_IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF_Vec(self, __TF_IDF_Dict):\n",
    "        wordDict = sorted(self.tf_dict.keys())\n",
    "        TF_IDF_vector = [0.0] * len(wordDict)\n",
    "\n",
    "        for i, term in enumerate(wordDict):\n",
    "            if term in __TF_IDF_Dict:\n",
    "                TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "        return TF_IDF_vector\n",
    "\n",
    "    def set_tf_idf_dict(self, data):\n",
    "        data['tweet_token'] = data['tweet_clean'].apply(self.__tokenize)\n",
    "        data[\"tf_dict\"] = data['tweet_token'].apply(self.__calc_TF_Dict)\n",
    "        self.tf_dict = self.__calc_count_Dict(data[\"tf_dict\"])\n",
    "        self.idf_dict = self.__calc_IDF_Dict(len(data),  self.tf_dict)\n",
    "\n",
    "    def calc_tf_idf(self, data):\n",
    "        data_token = data.apply(self.__tokenize)\n",
    "        data_tf_dict = data_token.apply(self.__calc_TF_Dict)\n",
    "        data_tfidf_dict = data_tf_dict.apply(self.__calc_TF_IDF)\n",
    "        tfidf_vector = [self.__calc_TF_IDF_Vec(row) for row in data_tfidf_dict]\n",
    "        return tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _predict(self, x_test):\n",
    "        # Calculate posterior for each class\n",
    "        posteriors = []\n",
    "        for idx, _ in enumerate(self._classes):\n",
    "            prior_c = np.log10(self._priors[idx])\n",
    "            conditionals_c = self._calc_conditionals(\n",
    "                self._conditionals[idx, :], x_test)\n",
    "            posteriors_c = np.sum(conditionals_c) + prior_c\n",
    "            posteriors.append(posteriors_c)\n",
    "\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _calc_conditionals(self, cls_cond, x_test):\n",
    "        return np.log(cls_cond) * x_test\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        m, n = X_train.shape\n",
    "        self._classes = np.unique(y_train)\n",
    "        print(self._classes)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # init: Prior & Conditional\n",
    "        self._priors = np.zeros(n_classes)\n",
    "        self._conditionals = np.zeros((n_classes, n))\n",
    "\n",
    "        # Get Prior and Conditional\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_train_c = X_train[c == y_train]\n",
    "            self._priors[idx] = X_train_c.shape[0] / m\n",
    "            print(self._priors[idx])\n",
    "            self._conditionals[idx, :] = ((X_train_c.sum(axis=0)) + self.alpha) / (np.sum(X_train_c.sum(axis=0) + self.alpha))\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return [self._predict(x_test) for x_test in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def performance(y_test, y_predict):\n",
    "    confus = confusion_matrix(y_test, predict)\n",
    "    tn, fp, fn, tp = confus.ravel()\n",
    "    accuracy = ((tp + tn)/(tp + tn + fp + fn))*100\n",
    "    precision = (tp / (tp + fp))*100\n",
    "    recall = (tp / (tp + fn))*100\n",
    "    print('accuracy =', accuracy)\n",
    "    print('precision =', precision)\n",
    "    print('recall =', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = 'web_dataset1.csv'\n",
    "preprocessing = Preprocessing()\n",
    "df_web = preprocessing.from_csv(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = TfidfFeature()\n",
    "feature.set_tf_idf_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['tweet_clean'], df['label'], test_size=1., shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train = feature.calc_tf_idf(x_train)\n",
    "ft_test = feature.calc_tf_idf(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(ft_train, y_train)\n",
    "predict = nb.predict(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
