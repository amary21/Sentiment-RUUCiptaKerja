{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_indonesia = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing(object):\n",
    "    def __init__(self):\n",
    "        self.factory = StemmerFactory()\n",
    "        self.stemmer = self.factory.create_stemmer()\n",
    "        self.kamus = self.__get_dictionary()\n",
    "\n",
    "    def __get_dictionary(self):\n",
    "        df = pd.read_csv('normalisasi.csv', sep=';')\n",
    "        dictlist = []\n",
    "        for row in df.values:\n",
    "            dictlist.append([row[0], row[1]])\n",
    "        return dictlist\n",
    "\n",
    "    def __remove_pattern(self, tweet: str, pattern):\n",
    "        r = re.findall(pattern, tweet)\n",
    "        for i in r:\n",
    "            tweet = re.sub(i, '', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def __remove_symbol(self, tweet: str):\n",
    "        tweet = self.__remove_url(tweet)\n",
    "        # get only alfabet\n",
    "        pattern = re.compile(r'\\b[^\\d\\W]+\\b')\n",
    "        newwords = []\n",
    "        for word in pattern.findall(tweet):\n",
    "            # case folding\n",
    "\n",
    "            word = word.lower()\n",
    "            for row in self.kamus:\n",
    "                key = row[0]\n",
    "                value = row[1]\n",
    "                if word == key:\n",
    "                    word = value\n",
    "                    break\n",
    "\n",
    "            word = word.replace(\"xyz\", \"\")\n",
    "            newwords.append(word)\n",
    "        return \" \".join(newwords)\n",
    "\n",
    "    def __remove_url(self, text):\n",
    "        # Remove additional white spaces\n",
    "        text = re.sub('[\\s]+', ' ', text)\n",
    "        text = re.sub('[\\n]+', ' ', text)\n",
    "        # remove all url\n",
    "        text = re.sub(r\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\", \"\", text)\n",
    "        # remove email\n",
    "        text = re.sub(r\"[\\w]+@[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # remove text twit\n",
    "        text = re.sub(r'((pic\\.[^\\s]+)|(twitter))', '', text)\n",
    "        # remove mentions, hashtag and web\n",
    "        text = re.sub(r\"(?:\\@|#|http?\\://)\\S+\", \"\", text)\n",
    "        # remove url\n",
    "        text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r'((https?://[^\\s]+))', '', text)\n",
    "        text = re.sub(r\"(pic[^\\s]+)|[\\w]+\\.[c][o][m]\", \"\", text)\n",
    "        # replace non ascii\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __remove_emojis(self, data):\n",
    "        emoj = re.compile(\"[\"\n",
    "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                          u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                          u\"\\U00002702-\\U000027B0\"  \n",
    "                          u\"\\U000024C2-\\U0001F251\"\n",
    "                          u\"\\U0001f926-\\U0001f937\"\n",
    "                          u\"\\U00010000-\\U0010ffff\"\n",
    "                          u\"\\u2640-\\u2642\"\n",
    "                          u\"\\u2600-\\u2B55\"\n",
    "                          u\"\\u200d\"\n",
    "                          u\"\\u23cf\"\n",
    "                          u\"\\u23e9\"\n",
    "                          u\"\\u231a\"\n",
    "                          u\"\\ufe0f\"  # dingbats\n",
    "                          u\"\\u3030\"\n",
    "                          \"]+\", re.UNICODE)\n",
    "        return re.sub(emoj, '', data)\n",
    "    \n",
    "    def __concate_duplicate(self, tweet):\n",
    "        term = \"a\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", tweet)\n",
    "        term = \"i\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"u\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"e\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"o\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        term = \"c\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"k\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"w\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "        term = \"h\" + r\"{3}\"\n",
    "        rep = re.sub(term, \" 3\", rep)\n",
    "\n",
    "        return rep\n",
    "\n",
    "    def __clean_tweets(self, tweet: str) -> str:\n",
    "        # tokenize tweets\n",
    "        \n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        \n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "        \n",
    "        tweets_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in stopwords_indonesia and  # remove stopwords\n",
    "                    word not in string.punctuation):  # remove punctuation\n",
    "                tweets_clean.append(word)\n",
    "\n",
    "        stem_word = self.stemmer.stem(\" \".join(tweets_clean))  # stemming word\n",
    "        return stem_word\n",
    "\n",
    "    def from_csv(self, file_name):\n",
    "        raw_data = pd.read_csv(file_name)\n",
    "#         df = pd.DataFrame(raw_data[['user_account', 'tweet', 'label']])\n",
    "        df = pd.DataFrame(raw_data[['Handle', 'Text','Emojis','Comments','Likes','Retweets']])\n",
    "\n",
    "        df['remove_user'] = np.vectorize(self.__remove_pattern)(df['Text'], \"(@\\\\w*)\")\n",
    "        df['remove_symbol'] = df[\"remove_user\"].apply(lambda x: np.vectorize(self.__remove_pattern)(x, \"(#\\\\w*)\"))\n",
    "        df['remove_duplicate_char'] = df['remove_symbol'].apply(self.__concate_duplicate)\n",
    "        df['remove_emojis'] = df['remove_duplicate_char'].apply(lambda x: self.__remove_emojis(self.__remove_symbol(x)))\n",
    "        \n",
    "        df.drop_duplicates(subset=\"remove_emojis\", keep='first', inplace=True)\n",
    "        \n",
    "        df['tweet_clean'] = df['remove_emojis'].apply(lambda x: self.__clean_tweets(x))\n",
    "        df = df.dropna(subset=[\"tweet_clean\"])\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            if row['tweet_clean'] == \"\":\n",
    "                df = df.drop(i)\n",
    "        \n",
    "        df['ruu'] = df['tweet_clean'].str.contains('rancang undang undang')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['ruu'] == False:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['ruu'], axis=1, inplace=True)\n",
    "        \n",
    "        df['thai_account'] = df['Text'].str.contains('#WhatsHappeninglnThailand')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['thai_account'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['thai_account'], axis=1, inplace=True)\n",
    "        \n",
    "        df['phil_account'] = df['Text'].str.contains('#WhatsHappeningInPhilippines')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['phil_account'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['phil_account'], axis=1, inplace=True)\n",
    "        \n",
    "        df['other_account'] = df['Text'].str.contains('#WhatIsHappeningInIndonesia')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['other_account'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['other_account'], axis=1, inplace=True)\n",
    "        \n",
    "        df['other_account'] = df['Text'].str.contains('#WhatsHappeningInIndonesia')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['other_account'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['other_account'], axis=1, inplace=True)\n",
    "        \n",
    "        df['other_tag'] = df['Text'].str.contains('#JunkTerrorBill')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['other_tag'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['other_tag'], axis=1, inplace=True)\n",
    "        \n",
    "        df['other_tag'] = df['Text'].str.contains('Junk Terror Bill')\n",
    "        for i, row in df.iterrows():\n",
    "            if row['other_tag'] == True:\n",
    "                df = df.drop(i)\n",
    "        df.drop(['other_tag'], axis=1, inplace=True)\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            if pd.isna(row['Retweets']) or pd.isna(row['Likes']):\n",
    "                df = df.drop(i)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfFeature(object):\n",
    " \n",
    "    def __init__(self):\n",
    "        self.tf_dict = {}\n",
    "        self.idf_dict = {}\n",
    "        \n",
    "\n",
    "    def __tokenize(self, tweet):\n",
    "        tokenizer = TweetTokenizer(\n",
    "            preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        return tokenizer.tokenize(tweet)\n",
    "\n",
    "    def __calc_TF_Dict(self, document):\n",
    "        TF_dict = {}\n",
    "        for term in document:\n",
    "            if term in TF_dict:\n",
    "                TF_dict[term] += 1\n",
    "            else:\n",
    "                TF_dict[term] = 1\n",
    "        return TF_dict\n",
    "\n",
    "    def __calc_count_Dict(self, tfDict):\n",
    "        count_DF = {}\n",
    "        for document in tfDict:\n",
    "            for term in document:\n",
    "                if term in count_DF:\n",
    "                    count_DF[term] += 1\n",
    "                else:\n",
    "                    count_DF[term] = 1\n",
    "        return count_DF\n",
    "\n",
    "    def __calc_IDF_Dict(self, __n_document, __DF):\n",
    "        IDF_Dict = {}\n",
    "        for term in __DF:\n",
    "            IDF_Dict[term] = np.log(__n_document / __DF[term])\n",
    "        return IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF(self, TF):\n",
    "        TF_IDF_Dict = {}\n",
    "        for key in TF:\n",
    "            TF_IDF_Dict[key] = self.tf_dict[key] * self.idf_dict[key]\n",
    "        return TF_IDF_Dict\n",
    "\n",
    "    def __calc_TF_IDF_Vec(self, __TF_IDF_Dict):\n",
    "        wordDict = sorted(self.tf_dict.keys())\n",
    "        TF_IDF_vector = [0.0] * len(wordDict)\n",
    "\n",
    "        for i, term in enumerate(wordDict):\n",
    "            if term in __TF_IDF_Dict:\n",
    "                TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "        return TF_IDF_vector\n",
    "\n",
    "    def set_tf_idf_dict(self, data):\n",
    "        data['tweet_token'] = data['tweet_clean'].apply(self.__tokenize)\n",
    "        data[\"tf_dict\"] = data['tweet_token'].apply(self.__calc_TF_Dict)\n",
    "        self.tf_dict = self.__calc_count_Dict(data[\"tf_dict\"])\n",
    "        self.idf_dict = self.__calc_IDF_Dict(len(data),  self.tf_dict)\n",
    "\n",
    "    def calc_tf_idf(self, data):\n",
    "        data_token = data.apply(self.__tokenize)\n",
    "        data_tf_dict = data_token.apply(self.__calc_TF_Dict)\n",
    "        data_tfidf_dict = data_tf_dict.apply(self.__calc_TF_IDF)\n",
    "        tfidf_vector = [self.__calc_TF_IDF_Vec(row) for row in data_tfidf_dict]\n",
    "        return tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _predict(self, x_test):\n",
    "        # Calculate posterior for each class\n",
    "        posteriors = []\n",
    "        for idx, _ in enumerate(self._classes):\n",
    "            prior_c = np.log10(self._priors[idx])\n",
    "            conditionals_c = self._calc_conditionals(\n",
    "                self._conditionals[idx, :], x_test)\n",
    "            posteriors_c = np.sum(conditionals_c) + prior_c\n",
    "            posteriors.append(posteriors_c)\n",
    "\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _calc_conditionals(self, cls_cond, x_test):\n",
    "        return np.log(cls_cond) * x_test\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.array(X_train)\n",
    "        m, n = X_train.shape\n",
    "        self._classes = np.unique(y_train)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # init: Prior & Conditional\n",
    "        self._priors = np.zeros(n_classes)\n",
    "        self._conditionals = np.zeros((n_classes, n))\n",
    "\n",
    "        # Get Prior and Conditional\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_train_c = X_train[c == y_train]\n",
    "            self._priors[idx] = X_train_c.shape[0] / m\n",
    "            self._conditionals[idx, :] = ((X_train_c.sum(axis=0)) + self.alpha) / (np.sum(X_train_c.sum(axis=0) + self.alpha))\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return [self._predict(x_test) for x_test in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def performance(y_test, y_predict):\n",
    "    confus = confusion_matrix(y_test, predict)\n",
    "    tn, fp, fn, tp = confus.ravel()\n",
    "    accuracy = ((tp + tn)/(tp + tn + fp + fn))*100\n",
    "    precision = (tp / (tp + fp))*100\n",
    "    recall = (tp / (tp + fn))*100\n",
    "    print('accuracy =', accuracy)\n",
    "    print('precision =', precision)\n",
    "    print('recall =', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = 'ruu_cipta_kerja_20102020_terbaru.csv'\n",
    "preprocessing = Preprocessing()\n",
    "df = preprocessing.from_csv(raw_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['thai_account'] = df['remove_user'].str.contains('#WhatsHappeninglnThailand')\n",
    "df\n",
    "# for i, row in df.iterrows():\n",
    "#     if row['thai_account']:\n",
    "#         df = df.drop(i)\n",
    "# df.drop(['thai_account'], axis=1, inplace=True)\n",
    "# df\n",
    "\n",
    "# df['phil_account'] = df['Text'].str.contains('#WhatsHappeningInPhilippines')\n",
    "# for i, row in df.iterrows():\n",
    "#     if row['phil_account'] == True:\n",
    "#         df = df.drop(i)\n",
    "# df.drop(['phil_account'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df\n",
    "for i, row in cdf.iterrows():\n",
    "#     print(pd.isna(row['Retweets']))\n",
    "    if pd.isna(row['Retweets']) or pd.isna(row['Likes']):\n",
    "        cdf = cdf.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_09062021_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cdf)\n",
    "cdf.to_csv('df_09062021.csv')\n",
    "# feature = TfidfFeature()\n",
    "# feature.set_tf_idf_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_export():\n",
    "    raw = pd.read_csv('drive_dataset.csv')\n",
    "    df = pd.DataFrame(raw[['tweet','label']])\n",
    "    dictlist = []\n",
    "    for row in df.values:\n",
    "        dictlist.append([row[0], row[1]])\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_export(tweet):\n",
    "    g_export = get_export()\n",
    "    for row in g_export:\n",
    "        key = row[0]\n",
    "        value = row[1]\n",
    "        if tweet == key:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentimen'] = df['Text'].apply(lambda x: clean_export(x))\n",
    "\n",
    "# for i, row in cdf.iterrows():\n",
    "#     if row['clean_export'] == True:\n",
    "#         cdf = cdf.drop(i)\n",
    "\n",
    "# cdf.drop(['clean_export'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def akun_asing(akun):\n",
    "    if akun == '@MoBMaB13':\n",
    "        return True\n",
    "    elif akun == '@inyourznx':\n",
    "        return True\n",
    "    elif akun == '@jaemin813_th':\n",
    "        return True\n",
    "    elif akun == '@cloudypolus':\n",
    "        return True\n",
    "    elif akun == '@__TakagiBot':\n",
    "        return True\n",
    "    elif akun == '@so_r_u_happynow':\n",
    "        return True\n",
    "    elif akun == '@inyourznx':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf['akun_asing'] = cdf['Handle'].apply(lambda x: akun_asing(x))\n",
    "\n",
    "for i, row in cdf.iterrows():\n",
    "    if row['akun_asing'] == True:\n",
    "        cdf = cdf.drop(i)\n",
    "    \n",
    "\n",
    "cdf.drop('akun_asing', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cdf.to_csv('df_09062021_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df['tweet_clean'], df['label'], test_size=1., shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train = feature.calc_tf_idf(x_train)\n",
    "ft_test = feature.calc_tf_idf(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(ft_train, y_train)\n",
    "predict = nb.predict(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('like_dataset_0.csv')\n",
    "df = pd.DataFrame(raw_data[['user_account', 'label','tweet','tweet_clean']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== percobaan ke-0 =========\n",
      "accuracy = 66.94677871148458\n",
      "precision = 80.21978021978022\n",
      "recall = 64.03508771929825\n",
      "\n",
      "====== percobaan ke-1 =========\n",
      "accuracy = 69.46778711484593\n",
      "precision = 85.07462686567165\n",
      "recall = 68.4\n",
      "\n",
      "====== percobaan ke-2 =========\n",
      "accuracy = 70.30812324929971\n",
      "precision = 83.73205741626795\n",
      "recall = 70.8502024291498\n",
      "\n",
      "====== percobaan ke-3 =========\n",
      "accuracy = 71.70868347338936\n",
      "precision = 84.40366972477065\n",
      "recall = 73.30677290836654\n",
      "\n",
      "====== percobaan ke-4 =========\n",
      "accuracy = 71.98879551820728\n",
      "precision = 86.09865470852019\n",
      "recall = 73.5632183908046\n",
      "\n",
      "====== percobaan ke-5 =========\n",
      "accuracy = 71.42857142857143\n",
      "precision = 83.61344537815127\n",
      "recall = 75.95419847328245\n",
      "\n",
      "====== percobaan ke-6 =========\n",
      "accuracy = 71.1484593837535\n",
      "precision = 83.33333333333334\n",
      "recall = 72.8744939271255\n",
      "\n",
      "====== percobaan ke-7 =========\n",
      "accuracy = 71.42857142857143\n",
      "precision = 82.58928571428571\n",
      "recall = 74.59677419354838\n",
      "\n",
      "====== percobaan ke-8 =========\n",
      "accuracy = 69.187675070028\n",
      "precision = 82.37885462555066\n",
      "recall = 72.76264591439688\n",
      "\n",
      "====== percobaan ke-9 =========\n",
      "accuracy = 69.187675070028\n",
      "precision = 85.25345622119815\n",
      "recall = 70.34220532319392\n",
      "\n",
      "====== percobaan ke-10 =========\n",
      "accuracy = 71.1484593837535\n",
      "precision = 84.16289592760181\n",
      "recall = 73.22834645669292\n",
      "\n",
      "====== percobaan ke-11 =========\n",
      "accuracy = 70.86834733893558\n",
      "precision = 82.71028037383178\n",
      "recall = 72.54098360655738\n",
      "\n",
      "====== percobaan ke-12 =========\n",
      "accuracy = 70.58823529411765\n",
      "precision = 82.01754385964912\n",
      "recall = 74.5019920318725\n",
      "\n",
      "====== percobaan ke-13 =========\n",
      "accuracy = 73.10924369747899\n",
      "precision = 86.23853211009175\n",
      "recall = 74.01574803149606\n",
      "\n",
      "====== percobaan ke-14 =========\n",
      "accuracy = 68.34733893557423\n",
      "precision = 81.25\n",
      "recall = 71.93675889328063\n",
      "\n",
      "====== percobaan ke-15 =========\n",
      "accuracy = 71.70868347338936\n",
      "precision = 86.11111111111111\n",
      "recall = 72.37354085603113\n",
      "\n",
      "====== percobaan ke-16 =========\n",
      "accuracy = 70.86834733893558\n",
      "precision = 83.98058252427184\n",
      "recall = 70.90163934426229\n",
      "\n",
      "====== percobaan ke-17 =========\n",
      "accuracy = 66.1064425770308\n",
      "precision = 80.54298642533936\n",
      "recall = 69.53125\n",
      "\n",
      "====== percobaan ke-18 =========\n",
      "accuracy = 73.38935574229691\n",
      "precision = 84.375\n",
      "recall = 75.90361445783132\n",
      "\n",
      "====== percobaan ke-19 =========\n",
      "accuracy = 70.02801120448179\n",
      "precision = 82.26600985221675\n",
      "recall = 70.16806722689076\n",
      "\n",
      "====== percobaan ke-20 =========\n",
      "accuracy = 73.38935574229691\n",
      "precision = 84.0\n",
      "recall = 76.20967741935483\n",
      "\n",
      "====== percobaan ke-21 =========\n",
      "accuracy = 71.42857142857143\n",
      "precision = 82.54716981132076\n",
      "recall = 72.91666666666666\n",
      "\n",
      "====== percobaan ke-22 =========\n",
      "accuracy = 67.78711484593838\n",
      "precision = 81.86046511627907\n",
      "recall = 69.84126984126983\n",
      "\n",
      "====== percobaan ke-23 =========\n",
      "accuracy = 71.1484593837535\n",
      "precision = 82.51121076233184\n",
      "recall = 74.19354838709677\n",
      "\n",
      "====== percobaan ke-24 =========\n",
      "accuracy = 73.94957983193278\n",
      "precision = 82.81938325991189\n",
      "recall = 77.68595041322314\n",
      "\n",
      "====== percobaan ke-25 =========\n",
      "accuracy = 69.74789915966386\n",
      "precision = 84.11214953271028\n",
      "recall = 70.86614173228347\n",
      "\n",
      "====== percobaan ke-26 =========\n",
      "accuracy = 70.86834733893558\n",
      "precision = 80.7017543859649\n",
      "recall = 75.40983606557377\n",
      "\n",
      "====== percobaan ke-27 =========\n",
      "accuracy = 65.82633053221288\n",
      "precision = 79.22705314009661\n",
      "recall = 67.48971193415639\n",
      "\n",
      "====== percobaan ke-28 =========\n",
      "accuracy = 72.26890756302521\n",
      "precision = 87.73584905660378\n",
      "recall = 71.81467181467181\n",
      "\n",
      "====== percobaan ke-29 =========\n",
      "accuracy = 74.50980392156863\n",
      "precision = 83.60655737704919\n",
      "recall = 80.0\n",
      "\n",
      "====== percobaan ke-30 =========\n",
      "accuracy = 68.90756302521008\n",
      "precision = 84.18604651162791\n",
      "recall = 70.15503875968993\n",
      "\n",
      "====== percobaan ke-31 =========\n",
      "accuracy = 69.74789915966386\n",
      "precision = 81.25\n",
      "recall = 73.38709677419355\n",
      "\n",
      "====== percobaan ke-32 =========\n",
      "accuracy = 73.38935574229691\n",
      "precision = 86.07594936708861\n",
      "recall = 76.69172932330827\n",
      "\n",
      "====== percobaan ke-33 =========\n",
      "accuracy = 66.1064425770308\n",
      "precision = 80.95238095238095\n",
      "recall = 67.72908366533864\n",
      "\n",
      "====== percobaan ke-34 =========\n",
      "accuracy = 69.187675070028\n",
      "precision = 83.40425531914893\n",
      "recall = 73.40823970037454\n",
      "\n",
      "====== percobaan ke-35 =========\n",
      "accuracy = 70.30812324929971\n",
      "precision = 87.56218905472637\n",
      "recall = 68.48249027237354\n",
      "\n",
      "====== percobaan ke-36 =========\n",
      "accuracy = 68.62745098039215\n",
      "precision = 81.03448275862068\n",
      "recall = 73.4375\n",
      "\n",
      "====== percobaan ke-37 =========\n",
      "accuracy = 70.30812324929971\n",
      "precision = 82.96943231441048\n",
      "recall = 73.92996108949417\n",
      "\n",
      "====== percobaan ke-38 =========\n",
      "accuracy = 70.02801120448179\n",
      "precision = 81.65137614678899\n",
      "recall = 72.6530612244898\n",
      "\n",
      "====== percobaan ke-39 =========\n",
      "accuracy = 68.34733893557423\n",
      "precision = 81.08108108108108\n",
      "recall = 71.71314741035857\n",
      "\n",
      "====== percobaan ke-40 =========\n",
      "accuracy = 72.26890756302521\n",
      "precision = 83.12236286919831\n",
      "recall = 76.953125\n",
      "\n",
      "====== percobaan ke-41 =========\n",
      "accuracy = 69.46778711484593\n",
      "precision = 84.88888888888889\n",
      "recall = 71.80451127819549\n",
      "\n",
      "====== percobaan ke-42 =========\n",
      "accuracy = 68.34733893557423\n",
      "precision = 83.9622641509434\n",
      "recall = 69.26070038910505\n",
      "\n",
      "====== percobaan ke-43 =========\n",
      "accuracy = 64.70588235294117\n",
      "precision = 79.46428571428571\n",
      "recall = 68.9922480620155\n",
      "\n",
      "====== percobaan ke-44 =========\n",
      "accuracy = 69.46778711484593\n",
      "precision = 83.94495412844036\n",
      "recall = 71.20622568093385\n",
      "\n",
      "====== percobaan ke-45 =========\n",
      "accuracy = 67.22689075630252\n",
      "precision = 80.82191780821918\n",
      "recall = 70.23809523809523\n",
      "\n",
      "====== percobaan ke-46 =========\n",
      "accuracy = 70.58823529411765\n",
      "precision = 85.58951965065502\n",
      "recall = 73.13432835820896\n",
      "\n",
      "====== percobaan ke-47 =========\n",
      "accuracy = 72.54901960784314\n",
      "precision = 85.83690987124464\n",
      "recall = 75.47169811320755\n",
      "\n",
      "====== percobaan ke-48 =========\n",
      "accuracy = 71.70868347338936\n",
      "precision = 82.17391304347827\n",
      "recall = 75.90361445783132\n",
      "\n",
      "====== percobaan ke-49 =========\n",
      "accuracy = 67.50700280112045\n",
      "precision = 77.37556561085974\n",
      "recall = 72.15189873417721\n",
      "\n",
      "====== percobaan ke-50 =========\n",
      "accuracy = 71.70868347338936\n",
      "precision = 84.82142857142857\n",
      "recall = 73.92996108949417\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 51):\n",
    "    idx = str(x)\n",
    "    raw_data = pd.read_csv('like_dataset_'+idx+'.csv')\n",
    "    df = pd.DataFrame(raw_data[['user_account', 'label','tweet','tweet_clean']])\n",
    "    feature = TfidfFeature()\n",
    "    feature.set_tf_idf_dict(df)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df['tweet_clean'], df['label'], test_size=0.2, shuffle=False)\n",
    "    ft_train = feature.calc_tf_idf(x_train)\n",
    "    ft_test = feature.calc_tf_idf(x_test)\n",
    "    \n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(ft_train, y_train)\n",
    "    predict = nb.predict(ft_test)\n",
    "    print('\\n====== percobaan ke-'+idx+' =========')\n",
    "    performance(y_test, predict)\n",
    "    \n",
    "    df = df.sample(frac = 1)\n",
    "    \n",
    "    new_x = x + 1\n",
    "    new_idx = str(new_x)\n",
    "    df.to_csv('like_dataset_'+new_idx+'.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
